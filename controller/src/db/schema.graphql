schema {
  query: query_root
  mutation: mutation_root
  subscription: subscription_root
}
"whether this query should be cached (Hasura Cloud only)"
directive @cached("measured in seconds" ttl: Int! = 60, "refresh the cache entry" refresh: Boolean! = false) on QUERY
input CreateRunInput {
  " An optional list of environment variables to set for the steps of the run container "
  environmentVariables: [StepEnvironmentVariable!]
  " The name of the run "
  name: String!
  """
   The simulation to which this run belongs.
  
  The user must own the simulation.
  """
  simulationId: uuid!
  " An optional list of timeouts to set for the steps of the run container "
  timeouts: [StepTimeout!]
}
"ordering argument of a cursor"
enum CursorOrdering {
  "ascending ordering of the cursor"
  ASC
  "descending ordering of the cursor"
  DESC
}
scalar Float
"""
Boolean expression to compare columns of type "float8". All fields are combined with logical 'AND'.
"""
input Float8ComparisonExp {
  _eq: float8
  _gt: float8
  _gte: float8
  _in: [float8!]
  _isNull: Boolean
  _lt: float8
  _lte: float8
  _neq: float8
  _nin: [float8!]
}
"""
Boolean expression to compare columns of type "Int". All fields are combined with logical 'AND'.
"""
input IntComparisonExp {
  _eq: Int
  _gt: Int
  _gte: Int
  _in: [Int!]
  _isNull: Boolean
  _lt: Int
  _lte: Int
  _neq: Int
  _nin: [Int!]
}
input JsonbCastExp {
  String: StringComparisonExp
}
"""
Boolean expression to compare columns of type "jsonb". All fields are combined with logical 'AND'.
"""
input JsonbComparisonExp {
  _cast: JsonbCastExp
  "is the column contained in the given json value"
  _containedIn: jsonb
  "does the column contain the given json value at the top level"
  _contains: jsonb
  _eq: jsonb
  _gt: jsonb
  _gte: jsonb
  "does the string exist as a top-level key in the column"
  _hasKey: String
  "do all of these strings exist as top-level keys in the column"
  _hasKeysAll: [String!]
  "do any of these strings exist as top-level keys in the column"
  _hasKeysAny: [String!]
  _in: [jsonb!]
  _isNull: Boolean
  _lt: jsonb
  _lte: jsonb
  _neq: jsonb
  _nin: [jsonb!]
}
"column ordering options"
enum OrderBy {
  "in ascending order, nulls last"
  ASC
  "in ascending order, nulls first"
  ASC_NULLS_FIRST
  "in ascending order, nulls last"
  ASC_NULLS_LAST
  "in descending order, nulls first"
  DESC
  "in descending order, nulls first"
  DESC_NULLS_FIRST
  "in descending order, nulls last"
  DESC_NULLS_LAST
}
type Run {
  Run: runs
  runId: uuid!
}
"Environment variables in runs"
type SimpipeEnvs {
  "UUID of the env, random by default"
  envId: uuid!
  "Name of the env, it is unique per step"
  name: String!
  "An object relationship"
  step: steps!
  "UUID of the step, must exist in the steps table"
  stepId: uuid!
  "Value of the env"
  value: String!
}
"""
aggregated selection of "simpipe.envs"
"""
type SimpipeEnvsAggregate {
  aggregate: SimpipeEnvsAggregateFields
  nodes: [SimpipeEnvs!]!
}
"""
aggregate fields of "simpipe.envs"
"""
type SimpipeEnvsAggregateFields {
  count(columns: [SimpipeEnvsSelectColumn!], distinct: Boolean): Int!
  max: SimpipeEnvsMaxFields
  min: SimpipeEnvsMinFields
}
"""
order by aggregate values of table "simpipe.envs"
"""
input SimpipeEnvsAggregateOrderBy {
  count: OrderBy
  max: simpipe_envs_max_order_by
  min: simpipe_envs_min_order_by
}
"""
input type for inserting array relation for remote table "simpipe.envs"
"""
input SimpipeEnvsArrRelInsertInput {
  data: [SimpipeEnvsInsertInput!]!
  "upsert condition"
  onConflict: SimpipeEnvsOnConflict
}
"""
Boolean expression to filter rows from the table "simpipe.envs". All fields are combined with a logical 'AND'.
"""
input SimpipeEnvsBoolExp {
  _and: [SimpipeEnvsBoolExp!]
  _not: SimpipeEnvsBoolExp
  _or: [SimpipeEnvsBoolExp!]
  envId: UuidComparisonExp
  name: StringComparisonExp
  step: stepsBoolExp
  stepId: UuidComparisonExp
  value: StringComparisonExp
}
"""
unique or primary key constraints on table "simpipe.envs"
"""
enum SimpipeEnvsConstraint {
  """
  unique or primary key constraint on columns "env_id"
  """
  envs_pkey
  """
  unique or primary key constraint on columns "name", "step_id"
  """
  envs_step_id_name_key
}
"""
input type for inserting data into table "simpipe.envs"
"""
input SimpipeEnvsInsertInput {
  "UUID of the env, random by default"
  envId: uuid
  "Name of the env, it is unique per step"
  name: String
  step: stepsObjRelInsertInput
  "UUID of the step, must exist in the steps table"
  stepId: uuid
  "Value of the env"
  value: String
}
"aggregate max on columns"
type SimpipeEnvsMaxFields {
  "UUID of the env, random by default"
  envId: uuid
  "Name of the env, it is unique per step"
  name: String
  "UUID of the step, must exist in the steps table"
  stepId: uuid
  "Value of the env"
  value: String
}
"aggregate min on columns"
type SimpipeEnvsMinFields {
  "UUID of the env, random by default"
  envId: uuid
  "Name of the env, it is unique per step"
  name: String
  "UUID of the step, must exist in the steps table"
  stepId: uuid
  "Value of the env"
  value: String
}
"""
response of any mutation on the table "simpipe.envs"
"""
type SimpipeEnvsMutationResponse {
  "number of rows affected by the mutation"
  affected_rows: Int!
  "data from the rows affected by the mutation"
  returning: [SimpipeEnvs!]!
}
"""
on_conflict condition type for table "simpipe.envs"
"""
input SimpipeEnvsOnConflict {
  constraint: SimpipeEnvsConstraint!
  update_columns: [SimpipeEnvsUpdateColumn!]! = []
  where: SimpipeEnvsBoolExp
}
"""
Ordering options when selecting data from "simpipe.envs".
"""
input SimpipeEnvsOrderBy {
  envId: OrderBy
  name: OrderBy
  step: stepsOrderBy
  stepId: OrderBy
  value: OrderBy
}
"primary key columns input for table: simpipe.envs"
input SimpipeEnvsPkColumnsInput {
  "UUID of the env, random by default"
  envId: uuid!
}
"""
select columns of table "simpipe.envs"
"""
enum SimpipeEnvsSelectColumn {
  "column name"
  envId
  "column name"
  name
  "column name"
  stepId
  "column name"
  value
}
"""
input type for updating data in table "simpipe.envs"
"""
input SimpipeEnvsSetInput {
  "UUID of the env, random by default"
  envId: uuid
  "Name of the env, it is unique per step"
  name: String
  "UUID of the step, must exist in the steps table"
  stepId: uuid
  "Value of the env"
  value: String
}
"""
Streaming cursor of the table "simpipe_envs"
"""
input SimpipeEnvsStreamCursorInput {
  "Stream column input with initial value"
  initialValue: SimpipeEnvsStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input SimpipeEnvsStreamCursorValueInput {
  "UUID of the env, random by default"
  envId: uuid
  "Name of the env, it is unique per step"
  name: String
  "UUID of the step, must exist in the steps table"
  stepId: uuid
  "Value of the env"
  value: String
}
"""
update columns of table "simpipe.envs"
"""
enum SimpipeEnvsUpdateColumn {
  "column name"
  envId
  "column name"
  name
  "column name"
  stepId
  "column name"
  value
}
input SimpipeEnvsUpdates {
  "sets the columns of the filtered rows to the given values"
  _set: SimpipeEnvsSetInput
  "filter the rows which have to be updated"
  where: SimpipeEnvsBoolExp!
}
"Logs of the runs"
type SimpipeLogs {
  "An object relationship"
  step: steps!
  "UUID of the step, must exist in the steps table"
  stepId: uuid!
  "Text of the log"
  text: String!
}
"""
aggregated selection of "simpipe.logs"
"""
type SimpipeLogsAggregate {
  aggregate: SimpipeLogsAggregateFields
  nodes: [SimpipeLogs!]!
}
"""
aggregate fields of "simpipe.logs"
"""
type SimpipeLogsAggregateFields {
  count(columns: [SimpipeLogsSelectColumn!], distinct: Boolean): Int!
  max: SimpipeLogsMaxFields
  min: SimpipeLogsMinFields
}
"""
Boolean expression to filter rows from the table "simpipe.logs". All fields are combined with a logical 'AND'.
"""
input SimpipeLogsBoolExp {
  _and: [SimpipeLogsBoolExp!]
  _not: SimpipeLogsBoolExp
  _or: [SimpipeLogsBoolExp!]
  step: stepsBoolExp
  stepId: UuidComparisonExp
  text: StringComparisonExp
}
"""
unique or primary key constraints on table "simpipe.logs"
"""
enum SimpipeLogsConstraint {
  """
  unique or primary key constraint on columns "step_id"
  """
  logs_pkey
}
"""
input type for inserting data into table "simpipe.logs"
"""
input SimpipeLogsInsertInput {
  step: stepsObjRelInsertInput
  "UUID of the step, must exist in the steps table"
  stepId: uuid
  "Text of the log"
  text: String
}
"aggregate max on columns"
type SimpipeLogsMaxFields {
  "UUID of the step, must exist in the steps table"
  stepId: uuid
  "Text of the log"
  text: String
}
"aggregate min on columns"
type SimpipeLogsMinFields {
  "UUID of the step, must exist in the steps table"
  stepId: uuid
  "Text of the log"
  text: String
}
"""
response of any mutation on the table "simpipe.logs"
"""
type SimpipeLogsMutationResponse {
  "number of rows affected by the mutation"
  affected_rows: Int!
  "data from the rows affected by the mutation"
  returning: [SimpipeLogs!]!
}
"""
input type for inserting object relation for remote table "simpipe.logs"
"""
input SimpipeLogsObjRelInsertInput {
  data: SimpipeLogsInsertInput!
  "upsert condition"
  onConflict: SimpipeLogsOnConflict
}
"""
on_conflict condition type for table "simpipe.logs"
"""
input SimpipeLogsOnConflict {
  constraint: SimpipeLogsConstraint!
  update_columns: [SimpipeLogsUpdateColumn!]! = []
  where: SimpipeLogsBoolExp
}
"""
Ordering options when selecting data from "simpipe.logs".
"""
input SimpipeLogsOrderBy {
  step: stepsOrderBy
  stepId: OrderBy
  text: OrderBy
}
"primary key columns input for table: simpipe.logs"
input SimpipeLogsPkColumnsInput {
  "UUID of the step, must exist in the steps table"
  stepId: uuid!
}
"""
select columns of table "simpipe.logs"
"""
enum SimpipeLogsSelectColumn {
  "column name"
  stepId
  "column name"
  text
}
"""
input type for updating data in table "simpipe.logs"
"""
input SimpipeLogsSetInput {
  "UUID of the step, must exist in the steps table"
  stepId: uuid
  "Text of the log"
  text: String
}
"""
Streaming cursor of the table "simpipe_logs"
"""
input SimpipeLogsStreamCursorInput {
  "Stream column input with initial value"
  initialValue: SimpipeLogsStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input SimpipeLogsStreamCursorValueInput {
  "UUID of the step, must exist in the steps table"
  stepId: uuid
  "Text of the log"
  text: String
}
"""
update columns of table "simpipe.logs"
"""
enum SimpipeLogsUpdateColumn {
  "column name"
  stepId
  "column name"
  text
}
input SimpipeLogsUpdates {
  "sets the columns of the filtered rows to the given values"
  _set: SimpipeLogsSetInput
  "filter the rows which have to be updated"
  where: SimpipeLogsBoolExp!
}
"Status of a run"
type SimpipeRunStatus {
  value: String!
}
"""
aggregated selection of "simpipe.run_status"
"""
type SimpipeRunStatusAggregate {
  aggregate: SimpipeRunStatusAggregateFields
  nodes: [SimpipeRunStatus!]!
}
"""
aggregate fields of "simpipe.run_status"
"""
type SimpipeRunStatusAggregateFields {
  count(columns: [SimpipeRunStatusSelectColumn!], distinct: Boolean): Int!
  max: SimpipeRunStatusMaxFields
  min: SimpipeRunStatusMinFields
}
"""
Boolean expression to filter rows from the table "simpipe.run_status". All fields are combined with a logical 'AND'.
"""
input SimpipeRunStatusBoolExp {
  _and: [SimpipeRunStatusBoolExp!]
  _not: SimpipeRunStatusBoolExp
  _or: [SimpipeRunStatusBoolExp!]
  value: StringComparisonExp
}
"""
unique or primary key constraints on table "simpipe.run_status"
"""
enum SimpipeRunStatusConstraint {
  """
  unique or primary key constraint on columns "value"
  """
  run_status_pkey
}
enum SimpipeRunStatusEnum {
  ACTIVE
  CANCELLED
  COMPLETED
  FAILED
  QUEUED
  WAITING
}
"""
Boolean expression to compare columns of type "SimpipeRunStatusEnum". All fields are combined with logical 'AND'.
"""
input SimpipeRunStatusEnumComparisonExp {
  _eq: SimpipeRunStatusEnum
  _in: [SimpipeRunStatusEnum!]
  _isNull: Boolean
  _neq: SimpipeRunStatusEnum
  _nin: [SimpipeRunStatusEnum!]
}
"""
input type for inserting data into table "simpipe.run_status"
"""
input SimpipeRunStatusInsertInput {
  value: String
}
"aggregate max on columns"
type SimpipeRunStatusMaxFields {
  value: String
}
"aggregate min on columns"
type SimpipeRunStatusMinFields {
  value: String
}
"""
response of any mutation on the table "simpipe.run_status"
"""
type SimpipeRunStatusMutationResponse {
  "number of rows affected by the mutation"
  affected_rows: Int!
  "data from the rows affected by the mutation"
  returning: [SimpipeRunStatus!]!
}
"""
on_conflict condition type for table "simpipe.run_status"
"""
input SimpipeRunStatusOnConflict {
  constraint: SimpipeRunStatusConstraint!
  update_columns: [SimpipeRunStatusUpdateColumn!]! = []
  where: SimpipeRunStatusBoolExp
}
"""
Ordering options when selecting data from "simpipe.run_status".
"""
input SimpipeRunStatusOrderBy {
  value: OrderBy
}
"primary key columns input for table: simpipe.run_status"
input SimpipeRunStatusPkColumnsInput {
  value: String!
}
"""
select columns of table "simpipe.run_status"
"""
enum SimpipeRunStatusSelectColumn {
  "column name"
  value
}
"""
input type for updating data in table "simpipe.run_status"
"""
input SimpipeRunStatusSetInput {
  value: String
}
"""
Streaming cursor of the table "simpipe_run_status"
"""
input SimpipeRunStatusStreamCursorInput {
  "Stream column input with initial value"
  initialValue: SimpipeRunStatusStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input SimpipeRunStatusStreamCursorValueInput {
  value: String
}
"""
update columns of table "simpipe.run_status"
"""
enum SimpipeRunStatusUpdateColumn {
  "column name"
  value
}
input SimpipeRunStatusUpdates {
  "sets the columns of the filtered rows to the given values"
  _set: SimpipeRunStatusSetInput
  "filter the rows which have to be updated"
  where: SimpipeRunStatusBoolExp!
}
"Status of a step"
type SimpipeStepStatus {
  value: String!
}
"""
aggregated selection of "simpipe.step_status"
"""
type SimpipeStepStatusAggregate {
  aggregate: SimpipeStepStatusAggregateFields
  nodes: [SimpipeStepStatus!]!
}
"""
aggregate fields of "simpipe.step_status"
"""
type SimpipeStepStatusAggregateFields {
  count(columns: [SimpipeStepStatusSelectColumn!], distinct: Boolean): Int!
  max: SimpipeStepStatusMaxFields
  min: SimpipeStepStatusMinFields
}
"""
Boolean expression to filter rows from the table "simpipe.step_status". All fields are combined with a logical 'AND'.
"""
input SimpipeStepStatusBoolExp {
  _and: [SimpipeStepStatusBoolExp!]
  _not: SimpipeStepStatusBoolExp
  _or: [SimpipeStepStatusBoolExp!]
  value: StringComparisonExp
}
"""
unique or primary key constraints on table "simpipe.step_status"
"""
enum SimpipeStepStatusConstraint {
  """
  unique or primary key constraint on columns "value"
  """
  step_status_pkey
}
enum SimpipeStepStatusEnum {
  ACTIVE
  CANCELLED
  COMPLETED
  FAILED
  QUEUED
}
"""
Boolean expression to compare columns of type "SimpipeStepStatusEnum". All fields are combined with logical 'AND'.
"""
input SimpipeStepStatusEnumComparisonExp {
  _eq: SimpipeStepStatusEnum
  _in: [SimpipeStepStatusEnum!]
  _isNull: Boolean
  _neq: SimpipeStepStatusEnum
  _nin: [SimpipeStepStatusEnum!]
}
"""
input type for inserting data into table "simpipe.step_status"
"""
input SimpipeStepStatusInsertInput {
  value: String
}
"aggregate max on columns"
type SimpipeStepStatusMaxFields {
  value: String
}
"aggregate min on columns"
type SimpipeStepStatusMinFields {
  value: String
}
"""
response of any mutation on the table "simpipe.step_status"
"""
type SimpipeStepStatusMutationResponse {
  "number of rows affected by the mutation"
  affected_rows: Int!
  "data from the rows affected by the mutation"
  returning: [SimpipeStepStatus!]!
}
"""
on_conflict condition type for table "simpipe.step_status"
"""
input SimpipeStepStatusOnConflict {
  constraint: SimpipeStepStatusConstraint!
  update_columns: [SimpipeStepStatusUpdateColumn!]! = []
  where: SimpipeStepStatusBoolExp
}
"""
Ordering options when selecting data from "simpipe.step_status".
"""
input SimpipeStepStatusOrderBy {
  value: OrderBy
}
"primary key columns input for table: simpipe.step_status"
input SimpipeStepStatusPkColumnsInput {
  value: String!
}
"""
select columns of table "simpipe.step_status"
"""
enum SimpipeStepStatusSelectColumn {
  "column name"
  value
}
"""
input type for updating data in table "simpipe.step_status"
"""
input SimpipeStepStatusSetInput {
  value: String
}
"""
Streaming cursor of the table "simpipe_step_status"
"""
input SimpipeStepStatusStreamCursorInput {
  "Stream column input with initial value"
  initialValue: SimpipeStepStatusStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input SimpipeStepStatusStreamCursorValueInput {
  value: String
}
"""
update columns of table "simpipe.step_status"
"""
enum SimpipeStepStatusUpdateColumn {
  "column name"
  value
}
input SimpipeStepStatusUpdates {
  "sets the columns of the filtered rows to the given values"
  _set: SimpipeStepStatusSetInput
  "filter the rows which have to be updated"
  where: SimpipeStepStatusBoolExp!
}
input StepEnvironmentVariable {
  " The name of the environment variable "
  name: String!
  " The name of the step to set the environment variable for "
  stepName: String!
  " The value of the environment variable "
  value: String!
}
input StepTimeout {
  " The name of the step to set the timeout for "
  stepName: String!
  " The timeout in seconds, must be greater than 0 and less than 86400 (24 hours) "
  timeout: Int!
}
"""
Boolean expression to compare columns of type "String". All fields are combined with logical 'AND'.
"""
input StringComparisonExp {
  _eq: String
  _gt: String
  _gte: String
  "does the column match the given case-insensitive pattern"
  _ilike: String
  _in: [String!]
  "does the column match the given POSIX regular expression, case insensitive"
  _iregex: String
  _isNull: Boolean
  "does the column match the given pattern"
  _like: String
  _lt: String
  _lte: String
  _neq: String
  "does the column NOT match the given case-insensitive pattern"
  _nilike: String
  _nin: [String!]
  "does the column NOT match the given POSIX regular expression, case insensitive"
  _niregex: String
  "does the column NOT match the given pattern"
  _nlike: String
  "does the column NOT match the given POSIX regular expression, case sensitive"
  _nregex: String
  "does the column NOT match the given SQL regular expression"
  _nsimilar: String
  "does the column match the given POSIX regular expression, case sensitive"
  _regex: String
  "does the column match the given SQL regular expression"
  _similar: String
}
"""
Boolean expression to compare columns of type "timestamp". All fields are combined with logical 'AND'.
"""
input TimestampComparisonExp {
  _eq: timestamp
  _gt: timestamp
  _gte: timestamp
  _in: [timestamp!]
  _isNull: Boolean
  _lt: timestamp
  _lte: timestamp
  _neq: timestamp
  _nin: [timestamp!]
}
"""
Boolean expression to compare columns of type "timestamptz". All fields are combined with logical 'AND'.
"""
input TimestamptzComparisonExp {
  _eq: timestamptz
  _gt: timestamptz
  _gte: timestamptz
  _in: [timestamptz!]
  _isNull: Boolean
  _lt: timestamptz
  _lte: timestamptz
  _neq: timestamptz
  _nin: [timestamptz!]
}
"""
Boolean expression to compare columns of type "uuid". All fields are combined with logical 'AND'.
"""
input UuidComparisonExp {
  _eq: uuid
  _gt: uuid
  _gte: uuid
  _in: [uuid!]
  _isNull: Boolean
  _lt: uuid
  _lte: uuid
  _neq: uuid
  _nin: [uuid!]
}
"""
columns and relationships of "simpipe.cpu"
"""
type cpu {
  "An object relationship"
  run: runs
  runId: String
  "An object relationship"
  simulation: simulations
  simulationId: String
  "An object relationship"
  step: steps
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
aggregated selection of "simpipe.cpu"
"""
type cpuAggregate {
  aggregate: cpuAggregateFields
  nodes: [cpu!]!
}
"""
aggregate fields of "simpipe.cpu"
"""
type cpuAggregateFields {
  avg: cpuAvgFields
  count(columns: [cpuSelectColumn!], distinct: Boolean): Int!
  max: cpuMaxFields
  min: cpuMinFields
  stddev: cpuStddevFields
  stddevPop: cpuStddev_popFields
  stddevSamp: cpuStddev_sampFields
  sum: cpuSumFields
  varPop: cpuVar_popFields
  varSamp: cpuVar_sampFields
  variance: cpuVarianceFields
}
"""
order by aggregate values of table "simpipe.cpu"
"""
input cpuAggregateOrderBy {
  avg: cpu_avg_order_by
  count: OrderBy
  max: cpu_max_order_by
  min: cpu_min_order_by
  stddev: cpu_stddev_order_by
  stddev_pop: cpu_stddev_pop_order_by
  stddev_samp: cpu_stddev_samp_order_by
  sum: cpu_sum_order_by
  var_pop: cpu_var_pop_order_by
  var_samp: cpu_var_samp_order_by
  variance: cpu_variance_order_by
}
"""
input type for inserting array relation for remote table "simpipe.cpu"
"""
input cpuArrRelInsertInput {
  data: [cpuInsertInput!]!
}
"aggregate avg on columns"
type cpuAvgFields {
  value: Float
}
"""
Boolean expression to filter rows from the table "simpipe.cpu". All fields are combined with a logical 'AND'.
"""
input cpuBoolExp {
  _and: [cpuBoolExp!]
  _not: cpuBoolExp
  _or: [cpuBoolExp!]
  run: runsBoolExp
  runId: StringComparisonExp
  simulation: simulationsBoolExp
  simulationId: StringComparisonExp
  step: stepsBoolExp
  stepId: StringComparisonExp
  time: TimestamptzComparisonExp
  userId: StringComparisonExp
  value: Float8ComparisonExp
}
"""
input type for inserting data into table "simpipe.cpu"
"""
input cpuInsertInput {
  run: runsObjRelInsertInput
  runId: String
  simulation: simulationsObjRelInsertInput
  simulationId: String
  step: stepsObjRelInsertInput
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate max on columns"
type cpuMaxFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate min on columns"
type cpuMinFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
Ordering options when selecting data from "simpipe.cpu".
"""
input cpuOrderBy {
  run: runsOrderBy
  runId: OrderBy
  simulation: simulationsOrderBy
  simulationId: OrderBy
  step: stepsOrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select columns of table "simpipe.cpu"
"""
enum cpuSelectColumn {
  "column name"
  runId
  "column name"
  simulationId
  "column name"
  stepId
  "column name"
  time
  "column name"
  userId
  "column name"
  value
}
"aggregate stddev on columns"
type cpuStddevFields {
  value: Float
}
"aggregate stddev_pop on columns"
type cpuStddev_popFields {
  value: Float
}
"aggregate stddev_samp on columns"
type cpuStddev_sampFields {
  value: Float
}
"""
Streaming cursor of the table "cpu"
"""
input cpuStreamCursorInput {
  "Stream column input with initial value"
  initialValue: cpuStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input cpuStreamCursorValueInput {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate sum on columns"
type cpuSumFields {
  value: float8
}
"aggregate var_pop on columns"
type cpuVar_popFields {
  value: Float
}
"aggregate var_samp on columns"
type cpuVar_sampFields {
  value: Float
}
"aggregate variance on columns"
type cpuVarianceFields {
  value: Float
}
input cpu_aggregate_bool_exp {
  avg: cpu_aggregate_bool_exp_avg
  corr: cpu_aggregate_bool_exp_corr
  count: cpu_aggregate_bool_exp_count
  covar_samp: cpu_aggregate_bool_exp_covar_samp
  max: cpu_aggregate_bool_exp_max
  min: cpu_aggregate_bool_exp_min
  stddev_samp: cpu_aggregate_bool_exp_stddev_samp
  sum: cpu_aggregate_bool_exp_sum
  var_samp: cpu_aggregate_bool_exp_var_samp
}
input cpu_aggregate_bool_exp_avg {
  arguments: cpu_select_column_cpu_aggregate_bool_exp_avg_arguments_columns!
  distinct: Boolean
  filter: cpuBoolExp
  predicate: Float8ComparisonExp!
}
input cpu_aggregate_bool_exp_corr {
  arguments: cpu_aggregate_bool_exp_corr_arguments!
  distinct: Boolean
  filter: cpuBoolExp
  predicate: Float8ComparisonExp!
}
input cpu_aggregate_bool_exp_corr_arguments {
  X: cpu_select_column_cpu_aggregate_bool_exp_corr_arguments_columns!
  Y: cpu_select_column_cpu_aggregate_bool_exp_corr_arguments_columns!
}
input cpu_aggregate_bool_exp_count {
  arguments: [cpuSelectColumn!]
  distinct: Boolean
  filter: cpuBoolExp
  predicate: IntComparisonExp!
}
input cpu_aggregate_bool_exp_covar_samp {
  arguments: cpu_aggregate_bool_exp_covar_samp_arguments!
  distinct: Boolean
  filter: cpuBoolExp
  predicate: Float8ComparisonExp!
}
input cpu_aggregate_bool_exp_covar_samp_arguments {
  X: cpu_select_column_cpu_aggregate_bool_exp_covar_samp_arguments_columns!
  Y: cpu_select_column_cpu_aggregate_bool_exp_covar_samp_arguments_columns!
}
input cpu_aggregate_bool_exp_max {
  arguments: cpu_select_column_cpu_aggregate_bool_exp_max_arguments_columns!
  distinct: Boolean
  filter: cpuBoolExp
  predicate: Float8ComparisonExp!
}
input cpu_aggregate_bool_exp_min {
  arguments: cpu_select_column_cpu_aggregate_bool_exp_min_arguments_columns!
  distinct: Boolean
  filter: cpuBoolExp
  predicate: Float8ComparisonExp!
}
input cpu_aggregate_bool_exp_stddev_samp {
  arguments: cpu_select_column_cpu_aggregate_bool_exp_stddev_samp_arguments_columns!
  distinct: Boolean
  filter: cpuBoolExp
  predicate: Float8ComparisonExp!
}
input cpu_aggregate_bool_exp_sum {
  arguments: cpu_select_column_cpu_aggregate_bool_exp_sum_arguments_columns!
  distinct: Boolean
  filter: cpuBoolExp
  predicate: Float8ComparisonExp!
}
input cpu_aggregate_bool_exp_var_samp {
  arguments: cpu_select_column_cpu_aggregate_bool_exp_var_samp_arguments_columns!
  distinct: Boolean
  filter: cpuBoolExp
  predicate: Float8ComparisonExp!
}
"""
order by avg() on columns of table "simpipe.cpu"
"""
input cpu_avg_order_by {
  value: OrderBy
}
"""
order by max() on columns of table "simpipe.cpu"
"""
input cpu_max_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
order by min() on columns of table "simpipe.cpu"
"""
input cpu_min_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select "cpu_aggregate_bool_exp_avg_arguments_columns" columns of table "simpipe.cpu"
"""
enum cpu_select_column_cpu_aggregate_bool_exp_avg_arguments_columns {
  "column name"
  value
}
"""
select "cpu_aggregate_bool_exp_corr_arguments_columns" columns of table "simpipe.cpu"
"""
enum cpu_select_column_cpu_aggregate_bool_exp_corr_arguments_columns {
  "column name"
  value
}
"""
select "cpu_aggregate_bool_exp_covar_samp_arguments_columns" columns of table "simpipe.cpu"
"""
enum cpu_select_column_cpu_aggregate_bool_exp_covar_samp_arguments_columns {
  "column name"
  value
}
"""
select "cpu_aggregate_bool_exp_max_arguments_columns" columns of table "simpipe.cpu"
"""
enum cpu_select_column_cpu_aggregate_bool_exp_max_arguments_columns {
  "column name"
  value
}
"""
select "cpu_aggregate_bool_exp_min_arguments_columns" columns of table "simpipe.cpu"
"""
enum cpu_select_column_cpu_aggregate_bool_exp_min_arguments_columns {
  "column name"
  value
}
"""
select "cpu_aggregate_bool_exp_stddev_samp_arguments_columns" columns of table "simpipe.cpu"
"""
enum cpu_select_column_cpu_aggregate_bool_exp_stddev_samp_arguments_columns {
  "column name"
  value
}
"""
select "cpu_aggregate_bool_exp_sum_arguments_columns" columns of table "simpipe.cpu"
"""
enum cpu_select_column_cpu_aggregate_bool_exp_sum_arguments_columns {
  "column name"
  value
}
"""
select "cpu_aggregate_bool_exp_var_samp_arguments_columns" columns of table "simpipe.cpu"
"""
enum cpu_select_column_cpu_aggregate_bool_exp_var_samp_arguments_columns {
  "column name"
  value
}
"""
order by stddev() on columns of table "simpipe.cpu"
"""
input cpu_stddev_order_by {
  value: OrderBy
}
"""
order by stddev_pop() on columns of table "simpipe.cpu"
"""
input cpu_stddev_pop_order_by {
  value: OrderBy
}
"""
order by stddev_samp() on columns of table "simpipe.cpu"
"""
input cpu_stddev_samp_order_by {
  value: OrderBy
}
"""
order by sum() on columns of table "simpipe.cpu"
"""
input cpu_sum_order_by {
  value: OrderBy
}
"""
order by var_pop() on columns of table "simpipe.cpu"
"""
input cpu_var_pop_order_by {
  value: OrderBy
}
"""
order by var_samp() on columns of table "simpipe.cpu"
"""
input cpu_var_samp_order_by {
  value: OrderBy
}
"""
order by variance() on columns of table "simpipe.cpu"
"""
input cpu_variance_order_by {
  value: OrderBy
}
scalar float8
"""
columns and relationships of "simpipe.fs_reads_merged"
"""
type fsReadsMerged {
  "An object relationship"
  run: runs
  runId: String
  "An object relationship"
  simulation: simulations
  simulationId: String
  "An object relationship"
  step: steps
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
aggregated selection of "simpipe.fs_reads_merged"
"""
type fsReadsMergedAggregate {
  aggregate: fsReadsMergedAggregateFields
  nodes: [fsReadsMerged!]!
}
"""
aggregate fields of "simpipe.fs_reads_merged"
"""
type fsReadsMergedAggregateFields {
  avg: fsReadsMergedAvgFields
  count(columns: [fsReadsMergedSelectColumn!], distinct: Boolean): Int!
  max: fsReadsMergedMaxFields
  min: fsReadsMergedMinFields
  stddev: fsReadsMergedStddevFields
  stddevPop: fsReadsMergedStddev_popFields
  stddevSamp: fsReadsMergedStddev_sampFields
  sum: fsReadsMergedSumFields
  varPop: fsReadsMergedVar_popFields
  varSamp: fsReadsMergedVar_sampFields
  variance: fsReadsMergedVarianceFields
}
"""
order by aggregate values of table "simpipe.fs_reads_merged"
"""
input fsReadsMergedAggregateOrderBy {
  avg: fsReadsMerged_avg_order_by
  count: OrderBy
  max: fsReadsMerged_max_order_by
  min: fsReadsMerged_min_order_by
  stddev: fsReadsMerged_stddev_order_by
  stddev_pop: fsReadsMerged_stddev_pop_order_by
  stddev_samp: fsReadsMerged_stddev_samp_order_by
  sum: fsReadsMerged_sum_order_by
  var_pop: fsReadsMerged_var_pop_order_by
  var_samp: fsReadsMerged_var_samp_order_by
  variance: fsReadsMerged_variance_order_by
}
"""
input type for inserting array relation for remote table "simpipe.fs_reads_merged"
"""
input fsReadsMergedArrRelInsertInput {
  data: [fsReadsMergedInsertInput!]!
}
"aggregate avg on columns"
type fsReadsMergedAvgFields {
  value: Float
}
"""
Boolean expression to filter rows from the table "simpipe.fs_reads_merged". All fields are combined with a logical 'AND'.
"""
input fsReadsMergedBoolExp {
  _and: [fsReadsMergedBoolExp!]
  _not: fsReadsMergedBoolExp
  _or: [fsReadsMergedBoolExp!]
  run: runsBoolExp
  runId: StringComparisonExp
  simulation: simulationsBoolExp
  simulationId: StringComparisonExp
  step: stepsBoolExp
  stepId: StringComparisonExp
  time: TimestamptzComparisonExp
  userId: StringComparisonExp
  value: Float8ComparisonExp
}
"""
input type for inserting data into table "simpipe.fs_reads_merged"
"""
input fsReadsMergedInsertInput {
  run: runsObjRelInsertInput
  runId: String
  simulation: simulationsObjRelInsertInput
  simulationId: String
  step: stepsObjRelInsertInput
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate max on columns"
type fsReadsMergedMaxFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate min on columns"
type fsReadsMergedMinFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
Ordering options when selecting data from "simpipe.fs_reads_merged".
"""
input fsReadsMergedOrderBy {
  run: runsOrderBy
  runId: OrderBy
  simulation: simulationsOrderBy
  simulationId: OrderBy
  step: stepsOrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select columns of table "simpipe.fs_reads_merged"
"""
enum fsReadsMergedSelectColumn {
  "column name"
  runId
  "column name"
  simulationId
  "column name"
  stepId
  "column name"
  time
  "column name"
  userId
  "column name"
  value
}
"aggregate stddev on columns"
type fsReadsMergedStddevFields {
  value: Float
}
"aggregate stddev_pop on columns"
type fsReadsMergedStddev_popFields {
  value: Float
}
"aggregate stddev_samp on columns"
type fsReadsMergedStddev_sampFields {
  value: Float
}
"""
Streaming cursor of the table "fsReadsMerged"
"""
input fsReadsMergedStreamCursorInput {
  "Stream column input with initial value"
  initialValue: fsReadsMergedStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input fsReadsMergedStreamCursorValueInput {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate sum on columns"
type fsReadsMergedSumFields {
  value: float8
}
"aggregate var_pop on columns"
type fsReadsMergedVar_popFields {
  value: Float
}
"aggregate var_samp on columns"
type fsReadsMergedVar_sampFields {
  value: Float
}
"aggregate variance on columns"
type fsReadsMergedVarianceFields {
  value: Float
}
input fsReadsMerged_aggregate_bool_exp {
  avg: fsReadsMerged_aggregate_bool_exp_avg
  corr: fsReadsMerged_aggregate_bool_exp_corr
  count: fsReadsMerged_aggregate_bool_exp_count
  covar_samp: fsReadsMerged_aggregate_bool_exp_covar_samp
  max: fsReadsMerged_aggregate_bool_exp_max
  min: fsReadsMerged_aggregate_bool_exp_min
  stddev_samp: fsReadsMerged_aggregate_bool_exp_stddev_samp
  sum: fsReadsMerged_aggregate_bool_exp_sum
  var_samp: fsReadsMerged_aggregate_bool_exp_var_samp
}
input fsReadsMerged_aggregate_bool_exp_avg {
  arguments: fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_avg_arguments_columns!
  distinct: Boolean
  filter: fsReadsMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsReadsMerged_aggregate_bool_exp_corr {
  arguments: fsReadsMerged_aggregate_bool_exp_corr_arguments!
  distinct: Boolean
  filter: fsReadsMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsReadsMerged_aggregate_bool_exp_corr_arguments {
  X: fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_corr_arguments_columns!
  Y: fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_corr_arguments_columns!
}
input fsReadsMerged_aggregate_bool_exp_count {
  arguments: [fsReadsMergedSelectColumn!]
  distinct: Boolean
  filter: fsReadsMergedBoolExp
  predicate: IntComparisonExp!
}
input fsReadsMerged_aggregate_bool_exp_covar_samp {
  arguments: fsReadsMerged_aggregate_bool_exp_covar_samp_arguments!
  distinct: Boolean
  filter: fsReadsMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsReadsMerged_aggregate_bool_exp_covar_samp_arguments {
  X: fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_covar_samp_arguments_columns!
  Y: fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_covar_samp_arguments_columns!
}
input fsReadsMerged_aggregate_bool_exp_max {
  arguments: fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_max_arguments_columns!
  distinct: Boolean
  filter: fsReadsMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsReadsMerged_aggregate_bool_exp_min {
  arguments: fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_min_arguments_columns!
  distinct: Boolean
  filter: fsReadsMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsReadsMerged_aggregate_bool_exp_stddev_samp {
  arguments: fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_stddev_samp_arguments_columns!
  distinct: Boolean
  filter: fsReadsMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsReadsMerged_aggregate_bool_exp_sum {
  arguments: fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_sum_arguments_columns!
  distinct: Boolean
  filter: fsReadsMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsReadsMerged_aggregate_bool_exp_var_samp {
  arguments: fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_var_samp_arguments_columns!
  distinct: Boolean
  filter: fsReadsMergedBoolExp
  predicate: Float8ComparisonExp!
}
"""
order by avg() on columns of table "simpipe.fs_reads_merged"
"""
input fsReadsMerged_avg_order_by {
  value: OrderBy
}
"""
order by max() on columns of table "simpipe.fs_reads_merged"
"""
input fsReadsMerged_max_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
order by min() on columns of table "simpipe.fs_reads_merged"
"""
input fsReadsMerged_min_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select "fsReadsMerged_aggregate_bool_exp_avg_arguments_columns" columns of table "simpipe.fs_reads_merged"
"""
enum fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_avg_arguments_columns {
  "column name"
  value
}
"""
select "fsReadsMerged_aggregate_bool_exp_corr_arguments_columns" columns of table "simpipe.fs_reads_merged"
"""
enum fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_corr_arguments_columns {
  "column name"
  value
}
"""
select "fsReadsMerged_aggregate_bool_exp_covar_samp_arguments_columns" columns of table "simpipe.fs_reads_merged"
"""
enum fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_covar_samp_arguments_columns {
  "column name"
  value
}
"""
select "fsReadsMerged_aggregate_bool_exp_max_arguments_columns" columns of table "simpipe.fs_reads_merged"
"""
enum fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_max_arguments_columns {
  "column name"
  value
}
"""
select "fsReadsMerged_aggregate_bool_exp_min_arguments_columns" columns of table "simpipe.fs_reads_merged"
"""
enum fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_min_arguments_columns {
  "column name"
  value
}
"""
select "fsReadsMerged_aggregate_bool_exp_stddev_samp_arguments_columns" columns of table "simpipe.fs_reads_merged"
"""
enum fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_stddev_samp_arguments_columns {
  "column name"
  value
}
"""
select "fsReadsMerged_aggregate_bool_exp_sum_arguments_columns" columns of table "simpipe.fs_reads_merged"
"""
enum fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_sum_arguments_columns {
  "column name"
  value
}
"""
select "fsReadsMerged_aggregate_bool_exp_var_samp_arguments_columns" columns of table "simpipe.fs_reads_merged"
"""
enum fsReadsMerged_select_column_fsReadsMerged_aggregate_bool_exp_var_samp_arguments_columns {
  "column name"
  value
}
"""
order by stddev() on columns of table "simpipe.fs_reads_merged"
"""
input fsReadsMerged_stddev_order_by {
  value: OrderBy
}
"""
order by stddev_pop() on columns of table "simpipe.fs_reads_merged"
"""
input fsReadsMerged_stddev_pop_order_by {
  value: OrderBy
}
"""
order by stddev_samp() on columns of table "simpipe.fs_reads_merged"
"""
input fsReadsMerged_stddev_samp_order_by {
  value: OrderBy
}
"""
order by sum() on columns of table "simpipe.fs_reads_merged"
"""
input fsReadsMerged_sum_order_by {
  value: OrderBy
}
"""
order by var_pop() on columns of table "simpipe.fs_reads_merged"
"""
input fsReadsMerged_var_pop_order_by {
  value: OrderBy
}
"""
order by var_samp() on columns of table "simpipe.fs_reads_merged"
"""
input fsReadsMerged_var_samp_order_by {
  value: OrderBy
}
"""
order by variance() on columns of table "simpipe.fs_reads_merged"
"""
input fsReadsMerged_variance_order_by {
  value: OrderBy
}
"""
columns and relationships of "simpipe.fs_writes_merged"
"""
type fsWritesMerged {
  "An object relationship"
  run: runs
  runId: String
  "An object relationship"
  simulation: simulations
  simulationId: String
  "An object relationship"
  step: steps
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
aggregated selection of "simpipe.fs_writes_merged"
"""
type fsWritesMergedAggregate {
  aggregate: fsWritesMergedAggregateFields
  nodes: [fsWritesMerged!]!
}
"""
aggregate fields of "simpipe.fs_writes_merged"
"""
type fsWritesMergedAggregateFields {
  avg: fsWritesMergedAvgFields
  count(columns: [fsWritesMergedSelectColumn!], distinct: Boolean): Int!
  max: fsWritesMergedMaxFields
  min: fsWritesMergedMinFields
  stddev: fsWritesMergedStddevFields
  stddevPop: fsWritesMergedStddev_popFields
  stddevSamp: fsWritesMergedStddev_sampFields
  sum: fsWritesMergedSumFields
  varPop: fsWritesMergedVar_popFields
  varSamp: fsWritesMergedVar_sampFields
  variance: fsWritesMergedVarianceFields
}
"""
order by aggregate values of table "simpipe.fs_writes_merged"
"""
input fsWritesMergedAggregateOrderBy {
  avg: fsWritesMerged_avg_order_by
  count: OrderBy
  max: fsWritesMerged_max_order_by
  min: fsWritesMerged_min_order_by
  stddev: fsWritesMerged_stddev_order_by
  stddev_pop: fsWritesMerged_stddev_pop_order_by
  stddev_samp: fsWritesMerged_stddev_samp_order_by
  sum: fsWritesMerged_sum_order_by
  var_pop: fsWritesMerged_var_pop_order_by
  var_samp: fsWritesMerged_var_samp_order_by
  variance: fsWritesMerged_variance_order_by
}
"""
input type for inserting array relation for remote table "simpipe.fs_writes_merged"
"""
input fsWritesMergedArrRelInsertInput {
  data: [fsWritesMergedInsertInput!]!
}
"aggregate avg on columns"
type fsWritesMergedAvgFields {
  value: Float
}
"""
Boolean expression to filter rows from the table "simpipe.fs_writes_merged". All fields are combined with a logical 'AND'.
"""
input fsWritesMergedBoolExp {
  _and: [fsWritesMergedBoolExp!]
  _not: fsWritesMergedBoolExp
  _or: [fsWritesMergedBoolExp!]
  run: runsBoolExp
  runId: StringComparisonExp
  simulation: simulationsBoolExp
  simulationId: StringComparisonExp
  step: stepsBoolExp
  stepId: StringComparisonExp
  time: TimestamptzComparisonExp
  userId: StringComparisonExp
  value: Float8ComparisonExp
}
"""
input type for inserting data into table "simpipe.fs_writes_merged"
"""
input fsWritesMergedInsertInput {
  run: runsObjRelInsertInput
  runId: String
  simulation: simulationsObjRelInsertInput
  simulationId: String
  step: stepsObjRelInsertInput
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate max on columns"
type fsWritesMergedMaxFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate min on columns"
type fsWritesMergedMinFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
Ordering options when selecting data from "simpipe.fs_writes_merged".
"""
input fsWritesMergedOrderBy {
  run: runsOrderBy
  runId: OrderBy
  simulation: simulationsOrderBy
  simulationId: OrderBy
  step: stepsOrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select columns of table "simpipe.fs_writes_merged"
"""
enum fsWritesMergedSelectColumn {
  "column name"
  runId
  "column name"
  simulationId
  "column name"
  stepId
  "column name"
  time
  "column name"
  userId
  "column name"
  value
}
"aggregate stddev on columns"
type fsWritesMergedStddevFields {
  value: Float
}
"aggregate stddev_pop on columns"
type fsWritesMergedStddev_popFields {
  value: Float
}
"aggregate stddev_samp on columns"
type fsWritesMergedStddev_sampFields {
  value: Float
}
"""
Streaming cursor of the table "fsWritesMerged"
"""
input fsWritesMergedStreamCursorInput {
  "Stream column input with initial value"
  initialValue: fsWritesMergedStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input fsWritesMergedStreamCursorValueInput {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate sum on columns"
type fsWritesMergedSumFields {
  value: float8
}
"aggregate var_pop on columns"
type fsWritesMergedVar_popFields {
  value: Float
}
"aggregate var_samp on columns"
type fsWritesMergedVar_sampFields {
  value: Float
}
"aggregate variance on columns"
type fsWritesMergedVarianceFields {
  value: Float
}
input fsWritesMerged_aggregate_bool_exp {
  avg: fsWritesMerged_aggregate_bool_exp_avg
  corr: fsWritesMerged_aggregate_bool_exp_corr
  count: fsWritesMerged_aggregate_bool_exp_count
  covar_samp: fsWritesMerged_aggregate_bool_exp_covar_samp
  max: fsWritesMerged_aggregate_bool_exp_max
  min: fsWritesMerged_aggregate_bool_exp_min
  stddev_samp: fsWritesMerged_aggregate_bool_exp_stddev_samp
  sum: fsWritesMerged_aggregate_bool_exp_sum
  var_samp: fsWritesMerged_aggregate_bool_exp_var_samp
}
input fsWritesMerged_aggregate_bool_exp_avg {
  arguments: fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_avg_arguments_columns!
  distinct: Boolean
  filter: fsWritesMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsWritesMerged_aggregate_bool_exp_corr {
  arguments: fsWritesMerged_aggregate_bool_exp_corr_arguments!
  distinct: Boolean
  filter: fsWritesMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsWritesMerged_aggregate_bool_exp_corr_arguments {
  X: fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_corr_arguments_columns!
  Y: fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_corr_arguments_columns!
}
input fsWritesMerged_aggregate_bool_exp_count {
  arguments: [fsWritesMergedSelectColumn!]
  distinct: Boolean
  filter: fsWritesMergedBoolExp
  predicate: IntComparisonExp!
}
input fsWritesMerged_aggregate_bool_exp_covar_samp {
  arguments: fsWritesMerged_aggregate_bool_exp_covar_samp_arguments!
  distinct: Boolean
  filter: fsWritesMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsWritesMerged_aggregate_bool_exp_covar_samp_arguments {
  X: fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_covar_samp_arguments_columns!
  Y: fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_covar_samp_arguments_columns!
}
input fsWritesMerged_aggregate_bool_exp_max {
  arguments: fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_max_arguments_columns!
  distinct: Boolean
  filter: fsWritesMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsWritesMerged_aggregate_bool_exp_min {
  arguments: fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_min_arguments_columns!
  distinct: Boolean
  filter: fsWritesMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsWritesMerged_aggregate_bool_exp_stddev_samp {
  arguments: fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_stddev_samp_arguments_columns!
  distinct: Boolean
  filter: fsWritesMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsWritesMerged_aggregate_bool_exp_sum {
  arguments: fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_sum_arguments_columns!
  distinct: Boolean
  filter: fsWritesMergedBoolExp
  predicate: Float8ComparisonExp!
}
input fsWritesMerged_aggregate_bool_exp_var_samp {
  arguments: fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_var_samp_arguments_columns!
  distinct: Boolean
  filter: fsWritesMergedBoolExp
  predicate: Float8ComparisonExp!
}
"""
order by avg() on columns of table "simpipe.fs_writes_merged"
"""
input fsWritesMerged_avg_order_by {
  value: OrderBy
}
"""
order by max() on columns of table "simpipe.fs_writes_merged"
"""
input fsWritesMerged_max_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
order by min() on columns of table "simpipe.fs_writes_merged"
"""
input fsWritesMerged_min_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select "fsWritesMerged_aggregate_bool_exp_avg_arguments_columns" columns of table "simpipe.fs_writes_merged"
"""
enum fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_avg_arguments_columns {
  "column name"
  value
}
"""
select "fsWritesMerged_aggregate_bool_exp_corr_arguments_columns" columns of table "simpipe.fs_writes_merged"
"""
enum fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_corr_arguments_columns {
  "column name"
  value
}
"""
select "fsWritesMerged_aggregate_bool_exp_covar_samp_arguments_columns" columns of table "simpipe.fs_writes_merged"
"""
enum fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_covar_samp_arguments_columns {
  "column name"
  value
}
"""
select "fsWritesMerged_aggregate_bool_exp_max_arguments_columns" columns of table "simpipe.fs_writes_merged"
"""
enum fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_max_arguments_columns {
  "column name"
  value
}
"""
select "fsWritesMerged_aggregate_bool_exp_min_arguments_columns" columns of table "simpipe.fs_writes_merged"
"""
enum fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_min_arguments_columns {
  "column name"
  value
}
"""
select "fsWritesMerged_aggregate_bool_exp_stddev_samp_arguments_columns" columns of table "simpipe.fs_writes_merged"
"""
enum fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_stddev_samp_arguments_columns {
  "column name"
  value
}
"""
select "fsWritesMerged_aggregate_bool_exp_sum_arguments_columns" columns of table "simpipe.fs_writes_merged"
"""
enum fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_sum_arguments_columns {
  "column name"
  value
}
"""
select "fsWritesMerged_aggregate_bool_exp_var_samp_arguments_columns" columns of table "simpipe.fs_writes_merged"
"""
enum fsWritesMerged_select_column_fsWritesMerged_aggregate_bool_exp_var_samp_arguments_columns {
  "column name"
  value
}
"""
order by stddev() on columns of table "simpipe.fs_writes_merged"
"""
input fsWritesMerged_stddev_order_by {
  value: OrderBy
}
"""
order by stddev_pop() on columns of table "simpipe.fs_writes_merged"
"""
input fsWritesMerged_stddev_pop_order_by {
  value: OrderBy
}
"""
order by stddev_samp() on columns of table "simpipe.fs_writes_merged"
"""
input fsWritesMerged_stddev_samp_order_by {
  value: OrderBy
}
"""
order by sum() on columns of table "simpipe.fs_writes_merged"
"""
input fsWritesMerged_sum_order_by {
  value: OrderBy
}
"""
order by var_pop() on columns of table "simpipe.fs_writes_merged"
"""
input fsWritesMerged_var_pop_order_by {
  value: OrderBy
}
"""
order by var_samp() on columns of table "simpipe.fs_writes_merged"
"""
input fsWritesMerged_var_samp_order_by {
  value: OrderBy
}
"""
order by variance() on columns of table "simpipe.fs_writes_merged"
"""
input fsWritesMerged_variance_order_by {
  value: OrderBy
}
scalar jsonb
"""
columns and relationships of "simpipe.memory_max_usage_bytes"
"""
type memoryMaxUsageBytes {
  "An object relationship"
  run: runs
  runId: String
  "An object relationship"
  simulation: simulations
  simulationId: String
  "An object relationship"
  step: steps
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
aggregated selection of "simpipe.memory_max_usage_bytes"
"""
type memoryMaxUsageBytesAggregate {
  aggregate: memoryMaxUsageBytesAggregateFields
  nodes: [memoryMaxUsageBytes!]!
}
"""
aggregate fields of "simpipe.memory_max_usage_bytes"
"""
type memoryMaxUsageBytesAggregateFields {
  avg: memoryMaxUsageBytesAvgFields
  count(columns: [memoryMaxUsageBytesSelectColumn!], distinct: Boolean): Int!
  max: memoryMaxUsageBytesMaxFields
  min: memoryMaxUsageBytesMinFields
  stddev: memoryMaxUsageBytesStddevFields
  stddevPop: memoryMaxUsageBytesStddev_popFields
  stddevSamp: memoryMaxUsageBytesStddev_sampFields
  sum: memoryMaxUsageBytesSumFields
  varPop: memoryMaxUsageBytesVar_popFields
  varSamp: memoryMaxUsageBytesVar_sampFields
  variance: memoryMaxUsageBytesVarianceFields
}
"""
order by aggregate values of table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytesAggregateOrderBy {
  avg: memoryMaxUsageBytes_avg_order_by
  count: OrderBy
  max: memoryMaxUsageBytes_max_order_by
  min: memoryMaxUsageBytes_min_order_by
  stddev: memoryMaxUsageBytes_stddev_order_by
  stddev_pop: memoryMaxUsageBytes_stddev_pop_order_by
  stddev_samp: memoryMaxUsageBytes_stddev_samp_order_by
  sum: memoryMaxUsageBytes_sum_order_by
  var_pop: memoryMaxUsageBytes_var_pop_order_by
  var_samp: memoryMaxUsageBytes_var_samp_order_by
  variance: memoryMaxUsageBytes_variance_order_by
}
"""
input type for inserting array relation for remote table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytesArrRelInsertInput {
  data: [memoryMaxUsageBytesInsertInput!]!
}
"aggregate avg on columns"
type memoryMaxUsageBytesAvgFields {
  value: Float
}
"""
Boolean expression to filter rows from the table "simpipe.memory_max_usage_bytes". All fields are combined with a logical 'AND'.
"""
input memoryMaxUsageBytesBoolExp {
  _and: [memoryMaxUsageBytesBoolExp!]
  _not: memoryMaxUsageBytesBoolExp
  _or: [memoryMaxUsageBytesBoolExp!]
  run: runsBoolExp
  runId: StringComparisonExp
  simulation: simulationsBoolExp
  simulationId: StringComparisonExp
  step: stepsBoolExp
  stepId: StringComparisonExp
  time: TimestamptzComparisonExp
  userId: StringComparisonExp
  value: Float8ComparisonExp
}
"""
input type for inserting data into table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytesInsertInput {
  run: runsObjRelInsertInput
  runId: String
  simulation: simulationsObjRelInsertInput
  simulationId: String
  step: stepsObjRelInsertInput
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate max on columns"
type memoryMaxUsageBytesMaxFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate min on columns"
type memoryMaxUsageBytesMinFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
Ordering options when selecting data from "simpipe.memory_max_usage_bytes".
"""
input memoryMaxUsageBytesOrderBy {
  run: runsOrderBy
  runId: OrderBy
  simulation: simulationsOrderBy
  simulationId: OrderBy
  step: stepsOrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select columns of table "simpipe.memory_max_usage_bytes"
"""
enum memoryMaxUsageBytesSelectColumn {
  "column name"
  runId
  "column name"
  simulationId
  "column name"
  stepId
  "column name"
  time
  "column name"
  userId
  "column name"
  value
}
"aggregate stddev on columns"
type memoryMaxUsageBytesStddevFields {
  value: Float
}
"aggregate stddev_pop on columns"
type memoryMaxUsageBytesStddev_popFields {
  value: Float
}
"aggregate stddev_samp on columns"
type memoryMaxUsageBytesStddev_sampFields {
  value: Float
}
"""
Streaming cursor of the table "memoryMaxUsageBytes"
"""
input memoryMaxUsageBytesStreamCursorInput {
  "Stream column input with initial value"
  initialValue: memoryMaxUsageBytesStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input memoryMaxUsageBytesStreamCursorValueInput {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate sum on columns"
type memoryMaxUsageBytesSumFields {
  value: float8
}
"aggregate var_pop on columns"
type memoryMaxUsageBytesVar_popFields {
  value: Float
}
"aggregate var_samp on columns"
type memoryMaxUsageBytesVar_sampFields {
  value: Float
}
"aggregate variance on columns"
type memoryMaxUsageBytesVarianceFields {
  value: Float
}
input memoryMaxUsageBytes_aggregate_bool_exp {
  avg: memoryMaxUsageBytes_aggregate_bool_exp_avg
  corr: memoryMaxUsageBytes_aggregate_bool_exp_corr
  count: memoryMaxUsageBytes_aggregate_bool_exp_count
  covar_samp: memoryMaxUsageBytes_aggregate_bool_exp_covar_samp
  max: memoryMaxUsageBytes_aggregate_bool_exp_max
  min: memoryMaxUsageBytes_aggregate_bool_exp_min
  stddev_samp: memoryMaxUsageBytes_aggregate_bool_exp_stddev_samp
  sum: memoryMaxUsageBytes_aggregate_bool_exp_sum
  var_samp: memoryMaxUsageBytes_aggregate_bool_exp_var_samp
}
input memoryMaxUsageBytes_aggregate_bool_exp_avg {
  arguments: memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_avg_arguments_columns!
  distinct: Boolean
  filter: memoryMaxUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryMaxUsageBytes_aggregate_bool_exp_corr {
  arguments: memoryMaxUsageBytes_aggregate_bool_exp_corr_arguments!
  distinct: Boolean
  filter: memoryMaxUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryMaxUsageBytes_aggregate_bool_exp_corr_arguments {
  X: memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_corr_arguments_columns!
  Y: memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_corr_arguments_columns!
}
input memoryMaxUsageBytes_aggregate_bool_exp_count {
  arguments: [memoryMaxUsageBytesSelectColumn!]
  distinct: Boolean
  filter: memoryMaxUsageBytesBoolExp
  predicate: IntComparisonExp!
}
input memoryMaxUsageBytes_aggregate_bool_exp_covar_samp {
  arguments: memoryMaxUsageBytes_aggregate_bool_exp_covar_samp_arguments!
  distinct: Boolean
  filter: memoryMaxUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryMaxUsageBytes_aggregate_bool_exp_covar_samp_arguments {
  X: memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_covar_samp_arguments_columns!
  Y: memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_covar_samp_arguments_columns!
}
input memoryMaxUsageBytes_aggregate_bool_exp_max {
  arguments: memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_max_arguments_columns!
  distinct: Boolean
  filter: memoryMaxUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryMaxUsageBytes_aggregate_bool_exp_min {
  arguments: memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_min_arguments_columns!
  distinct: Boolean
  filter: memoryMaxUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryMaxUsageBytes_aggregate_bool_exp_stddev_samp {
  arguments: memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_stddev_samp_arguments_columns!
  distinct: Boolean
  filter: memoryMaxUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryMaxUsageBytes_aggregate_bool_exp_sum {
  arguments: memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_sum_arguments_columns!
  distinct: Boolean
  filter: memoryMaxUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryMaxUsageBytes_aggregate_bool_exp_var_samp {
  arguments: memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_var_samp_arguments_columns!
  distinct: Boolean
  filter: memoryMaxUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
"""
order by avg() on columns of table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytes_avg_order_by {
  value: OrderBy
}
"""
order by max() on columns of table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytes_max_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
order by min() on columns of table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytes_min_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select "memoryMaxUsageBytes_aggregate_bool_exp_avg_arguments_columns" columns of table "simpipe.memory_max_usage_bytes"
"""
enum memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_avg_arguments_columns {
  "column name"
  value
}
"""
select "memoryMaxUsageBytes_aggregate_bool_exp_corr_arguments_columns" columns of table "simpipe.memory_max_usage_bytes"
"""
enum memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_corr_arguments_columns {
  "column name"
  value
}
"""
select "memoryMaxUsageBytes_aggregate_bool_exp_covar_samp_arguments_columns" columns of table "simpipe.memory_max_usage_bytes"
"""
enum memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_covar_samp_arguments_columns {
  "column name"
  value
}
"""
select "memoryMaxUsageBytes_aggregate_bool_exp_max_arguments_columns" columns of table "simpipe.memory_max_usage_bytes"
"""
enum memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_max_arguments_columns {
  "column name"
  value
}
"""
select "memoryMaxUsageBytes_aggregate_bool_exp_min_arguments_columns" columns of table "simpipe.memory_max_usage_bytes"
"""
enum memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_min_arguments_columns {
  "column name"
  value
}
"""
select "memoryMaxUsageBytes_aggregate_bool_exp_stddev_samp_arguments_columns" columns of table "simpipe.memory_max_usage_bytes"
"""
enum memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_stddev_samp_arguments_columns {
  "column name"
  value
}
"""
select "memoryMaxUsageBytes_aggregate_bool_exp_sum_arguments_columns" columns of table "simpipe.memory_max_usage_bytes"
"""
enum memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_sum_arguments_columns {
  "column name"
  value
}
"""
select "memoryMaxUsageBytes_aggregate_bool_exp_var_samp_arguments_columns" columns of table "simpipe.memory_max_usage_bytes"
"""
enum memoryMaxUsageBytes_select_column_memoryMaxUsageBytes_aggregate_bool_exp_var_samp_arguments_columns {
  "column name"
  value
}
"""
order by stddev() on columns of table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytes_stddev_order_by {
  value: OrderBy
}
"""
order by stddev_pop() on columns of table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytes_stddev_pop_order_by {
  value: OrderBy
}
"""
order by stddev_samp() on columns of table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytes_stddev_samp_order_by {
  value: OrderBy
}
"""
order by sum() on columns of table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytes_sum_order_by {
  value: OrderBy
}
"""
order by var_pop() on columns of table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytes_var_pop_order_by {
  value: OrderBy
}
"""
order by var_samp() on columns of table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytes_var_samp_order_by {
  value: OrderBy
}
"""
order by variance() on columns of table "simpipe.memory_max_usage_bytes"
"""
input memoryMaxUsageBytes_variance_order_by {
  value: OrderBy
}
"""
columns and relationships of "simpipe.memory_usage_bytes"
"""
type memoryUsageBytes {
  "An object relationship"
  run: runs
  runId: String
  "An object relationship"
  simulation: simulations
  simulationId: String
  "An object relationship"
  step: steps
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
aggregated selection of "simpipe.memory_usage_bytes"
"""
type memoryUsageBytesAggregate {
  aggregate: memoryUsageBytesAggregateFields
  nodes: [memoryUsageBytes!]!
}
"""
aggregate fields of "simpipe.memory_usage_bytes"
"""
type memoryUsageBytesAggregateFields {
  avg: memoryUsageBytesAvgFields
  count(columns: [memoryUsageBytesSelectColumn!], distinct: Boolean): Int!
  max: memoryUsageBytesMaxFields
  min: memoryUsageBytesMinFields
  stddev: memoryUsageBytesStddevFields
  stddevPop: memoryUsageBytesStddev_popFields
  stddevSamp: memoryUsageBytesStddev_sampFields
  sum: memoryUsageBytesSumFields
  varPop: memoryUsageBytesVar_popFields
  varSamp: memoryUsageBytesVar_sampFields
  variance: memoryUsageBytesVarianceFields
}
"""
order by aggregate values of table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytesAggregateOrderBy {
  avg: memoryUsageBytes_avg_order_by
  count: OrderBy
  max: memoryUsageBytes_max_order_by
  min: memoryUsageBytes_min_order_by
  stddev: memoryUsageBytes_stddev_order_by
  stddev_pop: memoryUsageBytes_stddev_pop_order_by
  stddev_samp: memoryUsageBytes_stddev_samp_order_by
  sum: memoryUsageBytes_sum_order_by
  var_pop: memoryUsageBytes_var_pop_order_by
  var_samp: memoryUsageBytes_var_samp_order_by
  variance: memoryUsageBytes_variance_order_by
}
"""
input type for inserting array relation for remote table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytesArrRelInsertInput {
  data: [memoryUsageBytesInsertInput!]!
}
"aggregate avg on columns"
type memoryUsageBytesAvgFields {
  value: Float
}
"""
Boolean expression to filter rows from the table "simpipe.memory_usage_bytes". All fields are combined with a logical 'AND'.
"""
input memoryUsageBytesBoolExp {
  _and: [memoryUsageBytesBoolExp!]
  _not: memoryUsageBytesBoolExp
  _or: [memoryUsageBytesBoolExp!]
  run: runsBoolExp
  runId: StringComparisonExp
  simulation: simulationsBoolExp
  simulationId: StringComparisonExp
  step: stepsBoolExp
  stepId: StringComparisonExp
  time: TimestamptzComparisonExp
  userId: StringComparisonExp
  value: Float8ComparisonExp
}
"""
input type for inserting data into table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytesInsertInput {
  run: runsObjRelInsertInput
  runId: String
  simulation: simulationsObjRelInsertInput
  simulationId: String
  step: stepsObjRelInsertInput
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate max on columns"
type memoryUsageBytesMaxFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate min on columns"
type memoryUsageBytesMinFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
Ordering options when selecting data from "simpipe.memory_usage_bytes".
"""
input memoryUsageBytesOrderBy {
  run: runsOrderBy
  runId: OrderBy
  simulation: simulationsOrderBy
  simulationId: OrderBy
  step: stepsOrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select columns of table "simpipe.memory_usage_bytes"
"""
enum memoryUsageBytesSelectColumn {
  "column name"
  runId
  "column name"
  simulationId
  "column name"
  stepId
  "column name"
  time
  "column name"
  userId
  "column name"
  value
}
"aggregate stddev on columns"
type memoryUsageBytesStddevFields {
  value: Float
}
"aggregate stddev_pop on columns"
type memoryUsageBytesStddev_popFields {
  value: Float
}
"aggregate stddev_samp on columns"
type memoryUsageBytesStddev_sampFields {
  value: Float
}
"""
Streaming cursor of the table "memoryUsageBytes"
"""
input memoryUsageBytesStreamCursorInput {
  "Stream column input with initial value"
  initialValue: memoryUsageBytesStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input memoryUsageBytesStreamCursorValueInput {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate sum on columns"
type memoryUsageBytesSumFields {
  value: float8
}
"aggregate var_pop on columns"
type memoryUsageBytesVar_popFields {
  value: Float
}
"aggregate var_samp on columns"
type memoryUsageBytesVar_sampFields {
  value: Float
}
"aggregate variance on columns"
type memoryUsageBytesVarianceFields {
  value: Float
}
input memoryUsageBytes_aggregate_bool_exp {
  avg: memoryUsageBytes_aggregate_bool_exp_avg
  corr: memoryUsageBytes_aggregate_bool_exp_corr
  count: memoryUsageBytes_aggregate_bool_exp_count
  covar_samp: memoryUsageBytes_aggregate_bool_exp_covar_samp
  max: memoryUsageBytes_aggregate_bool_exp_max
  min: memoryUsageBytes_aggregate_bool_exp_min
  stddev_samp: memoryUsageBytes_aggregate_bool_exp_stddev_samp
  sum: memoryUsageBytes_aggregate_bool_exp_sum
  var_samp: memoryUsageBytes_aggregate_bool_exp_var_samp
}
input memoryUsageBytes_aggregate_bool_exp_avg {
  arguments: memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_avg_arguments_columns!
  distinct: Boolean
  filter: memoryUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryUsageBytes_aggregate_bool_exp_corr {
  arguments: memoryUsageBytes_aggregate_bool_exp_corr_arguments!
  distinct: Boolean
  filter: memoryUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryUsageBytes_aggregate_bool_exp_corr_arguments {
  X: memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_corr_arguments_columns!
  Y: memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_corr_arguments_columns!
}
input memoryUsageBytes_aggregate_bool_exp_count {
  arguments: [memoryUsageBytesSelectColumn!]
  distinct: Boolean
  filter: memoryUsageBytesBoolExp
  predicate: IntComparisonExp!
}
input memoryUsageBytes_aggregate_bool_exp_covar_samp {
  arguments: memoryUsageBytes_aggregate_bool_exp_covar_samp_arguments!
  distinct: Boolean
  filter: memoryUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryUsageBytes_aggregate_bool_exp_covar_samp_arguments {
  X: memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_covar_samp_arguments_columns!
  Y: memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_covar_samp_arguments_columns!
}
input memoryUsageBytes_aggregate_bool_exp_max {
  arguments: memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_max_arguments_columns!
  distinct: Boolean
  filter: memoryUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryUsageBytes_aggregate_bool_exp_min {
  arguments: memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_min_arguments_columns!
  distinct: Boolean
  filter: memoryUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryUsageBytes_aggregate_bool_exp_stddev_samp {
  arguments: memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_stddev_samp_arguments_columns!
  distinct: Boolean
  filter: memoryUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryUsageBytes_aggregate_bool_exp_sum {
  arguments: memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_sum_arguments_columns!
  distinct: Boolean
  filter: memoryUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
input memoryUsageBytes_aggregate_bool_exp_var_samp {
  arguments: memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_var_samp_arguments_columns!
  distinct: Boolean
  filter: memoryUsageBytesBoolExp
  predicate: Float8ComparisonExp!
}
"""
order by avg() on columns of table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytes_avg_order_by {
  value: OrderBy
}
"""
order by max() on columns of table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytes_max_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
order by min() on columns of table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytes_min_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select "memoryUsageBytes_aggregate_bool_exp_avg_arguments_columns" columns of table "simpipe.memory_usage_bytes"
"""
enum memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_avg_arguments_columns {
  "column name"
  value
}
"""
select "memoryUsageBytes_aggregate_bool_exp_corr_arguments_columns" columns of table "simpipe.memory_usage_bytes"
"""
enum memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_corr_arguments_columns {
  "column name"
  value
}
"""
select "memoryUsageBytes_aggregate_bool_exp_covar_samp_arguments_columns" columns of table "simpipe.memory_usage_bytes"
"""
enum memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_covar_samp_arguments_columns {
  "column name"
  value
}
"""
select "memoryUsageBytes_aggregate_bool_exp_max_arguments_columns" columns of table "simpipe.memory_usage_bytes"
"""
enum memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_max_arguments_columns {
  "column name"
  value
}
"""
select "memoryUsageBytes_aggregate_bool_exp_min_arguments_columns" columns of table "simpipe.memory_usage_bytes"
"""
enum memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_min_arguments_columns {
  "column name"
  value
}
"""
select "memoryUsageBytes_aggregate_bool_exp_stddev_samp_arguments_columns" columns of table "simpipe.memory_usage_bytes"
"""
enum memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_stddev_samp_arguments_columns {
  "column name"
  value
}
"""
select "memoryUsageBytes_aggregate_bool_exp_sum_arguments_columns" columns of table "simpipe.memory_usage_bytes"
"""
enum memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_sum_arguments_columns {
  "column name"
  value
}
"""
select "memoryUsageBytes_aggregate_bool_exp_var_samp_arguments_columns" columns of table "simpipe.memory_usage_bytes"
"""
enum memoryUsageBytes_select_column_memoryUsageBytes_aggregate_bool_exp_var_samp_arguments_columns {
  "column name"
  value
}
"""
order by stddev() on columns of table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytes_stddev_order_by {
  value: OrderBy
}
"""
order by stddev_pop() on columns of table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytes_stddev_pop_order_by {
  value: OrderBy
}
"""
order by stddev_samp() on columns of table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytes_stddev_samp_order_by {
  value: OrderBy
}
"""
order by sum() on columns of table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytes_sum_order_by {
  value: OrderBy
}
"""
order by var_pop() on columns of table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytes_var_pop_order_by {
  value: OrderBy
}
"""
order by var_samp() on columns of table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytes_var_samp_order_by {
  value: OrderBy
}
"""
order by variance() on columns of table "simpipe.memory_usage_bytes"
"""
input memoryUsageBytes_variance_order_by {
  value: OrderBy
}
"mutation root"
type mutation_root {
  " Cancel a run, if the run is running it will be stopped "
  cancelRun(runId: uuid!): Run
  " Create a run, but does not start it "
  createRun(run: CreateRunInput!): Run!
  """
  insert a single row into the table: "simpipe.simulations"
  """
  createSimulation("the row to be inserted" object: simulationsInsertInput!, "upsert condition" onConflict: simulationsOnConflict): simulations
  """
  insert data into the table: "simpipe.simulations"
  """
  createSimulations("the rows to be inserted" objects: [simulationsInsertInput!]!, "upsert condition" onConflict: simulationsOnConflict): simulationsMutationResponse
  """
  delete data from the table: "simpipe.runs"
  """
  deleteRuns("filter the rows which have to be deleted" where: runsBoolExp!): runsMutationResponse
  """
  delete single row from the table: "simpipe.runs"
  """
  deleteRunsByPk("UUID of the run, random by default" runId: uuid!): runs
  """
  delete data from the table: "simpipe.envs"
  """
  deleteSimpipeEnvs("filter the rows which have to be deleted" where: SimpipeEnvsBoolExp!): SimpipeEnvsMutationResponse
  """
  delete single row from the table: "simpipe.envs"
  """
  deleteSimpipeEnvsByPk("UUID of the env, random by default" envId: uuid!): SimpipeEnvs
  """
  delete data from the table: "simpipe.logs"
  """
  deleteSimpipeLogs("filter the rows which have to be deleted" where: SimpipeLogsBoolExp!): SimpipeLogsMutationResponse
  """
  delete single row from the table: "simpipe.logs"
  """
  deleteSimpipeLogsByPk("UUID of the step, must exist in the steps table" stepId: uuid!): SimpipeLogs
  """
  delete data from the table: "simpipe.run_status"
  """
  deleteSimpipeRunStatus("filter the rows which have to be deleted" where: SimpipeRunStatusBoolExp!): SimpipeRunStatusMutationResponse
  """
  delete single row from the table: "simpipe.run_status"
  """
  deleteSimpipeRunStatusByPk(value: String!): SimpipeRunStatus
  """
  delete data from the table: "simpipe.step_status"
  """
  deleteSimpipeStepStatus("filter the rows which have to be deleted" where: SimpipeStepStatusBoolExp!): SimpipeStepStatusMutationResponse
  """
  delete single row from the table: "simpipe.step_status"
  """
  deleteSimpipeStepStatusByPk(value: String!): SimpipeStepStatus
  """
  delete single row from the table: "simpipe.simulations"
  """
  deleteSimulation("UUID of the simulation, random by default" simulationId: uuid!): simulations
  """
  delete data from the table: "simpipe.simulations"
  """
  deleteSimulations("filter the rows which have to be deleted" where: simulationsBoolExp!): simulationsMutationResponse
  """
  delete data from the table: "simpipe.steps"
  """
  deleteSteps("filter the rows which have to be deleted" where: stepsBoolExp!): stepsMutationResponse
  """
  delete single row from the table: "simpipe.steps"
  """
  deleteStepsByPk("UUID of the step, random by default" stepId: uuid!): steps
  """
  insert data into the table: "simpipe.runs"
  """
  insertRuns("the rows to be inserted" objects: [runsInsertInput!]!, "upsert condition" onConflict: runsOnConflict): runsMutationResponse
  """
  insert a single row into the table: "simpipe.runs"
  """
  insertRunsOne("the row to be inserted" object: runsInsertInput!, "upsert condition" onConflict: runsOnConflict): runs
  """
  insert data into the table: "simpipe.envs"
  """
  insertSimpipeEnvs("the rows to be inserted" objects: [SimpipeEnvsInsertInput!]!, "upsert condition" onConflict: SimpipeEnvsOnConflict): SimpipeEnvsMutationResponse
  """
  insert a single row into the table: "simpipe.envs"
  """
  insertSimpipeEnvsOne("the row to be inserted" object: SimpipeEnvsInsertInput!, "upsert condition" onConflict: SimpipeEnvsOnConflict): SimpipeEnvs
  """
  insert data into the table: "simpipe.logs"
  """
  insertSimpipeLogs("the rows to be inserted" objects: [SimpipeLogsInsertInput!]!, "upsert condition" onConflict: SimpipeLogsOnConflict): SimpipeLogsMutationResponse
  """
  insert a single row into the table: "simpipe.logs"
  """
  insertSimpipeLogsOne("the row to be inserted" object: SimpipeLogsInsertInput!, "upsert condition" onConflict: SimpipeLogsOnConflict): SimpipeLogs
  """
  insert data into the table: "simpipe.run_status"
  """
  insertSimpipeRunStatus("the rows to be inserted" objects: [SimpipeRunStatusInsertInput!]!, "upsert condition" onConflict: SimpipeRunStatusOnConflict): SimpipeRunStatusMutationResponse
  """
  insert a single row into the table: "simpipe.run_status"
  """
  insertSimpipeRunStatusOne("the row to be inserted" object: SimpipeRunStatusInsertInput!, "upsert condition" onConflict: SimpipeRunStatusOnConflict): SimpipeRunStatus
  """
  insert data into the table: "simpipe.step_status"
  """
  insertSimpipeStepStatus("the rows to be inserted" objects: [SimpipeStepStatusInsertInput!]!, "upsert condition" onConflict: SimpipeStepStatusOnConflict): SimpipeStepStatusMutationResponse
  """
  insert a single row into the table: "simpipe.step_status"
  """
  insertSimpipeStepStatusOne("the row to be inserted" object: SimpipeStepStatusInsertInput!, "upsert condition" onConflict: SimpipeStepStatusOnConflict): SimpipeStepStatus
  """
  insert data into the table: "simpipe.steps"
  """
  insertSteps("the rows to be inserted" objects: [stepsInsertInput!]!, "upsert condition" onConflict: stepsOnConflict): stepsMutationResponse
  """
  insert a single row into the table: "simpipe.steps"
  """
  insertStepsOne("the row to be inserted" object: stepsInsertInput!, "upsert condition" onConflict: stepsOnConflict): steps
  " Start a run, if other runs are running this run will wait in the queue "
  startRun(runId: uuid!): Run
  """
  update data of the table: "simpipe.runs"
  """
  updateRuns("sets the columns of the filtered rows to the given values" _set: runsSetInput, "filter the rows which have to be updated" where: runsBoolExp!): runsMutationResponse
  """
  update single row of the table: "simpipe.runs"
  """
  updateRunsByPk("sets the columns of the filtered rows to the given values" _set: runsSetInput, pk_columns: runsPkColumnsInput!): runs
  """
  update multiples rows of table: "simpipe.runs"
  """
  updateRunsMany("updates to execute, in order" updates: [runsUpdates!]!): [runsMutationResponse]
  """
  update data of the table: "simpipe.envs"
  """
  updateSimpipeEnvs("sets the columns of the filtered rows to the given values" _set: SimpipeEnvsSetInput, "filter the rows which have to be updated" where: SimpipeEnvsBoolExp!): SimpipeEnvsMutationResponse
  """
  update single row of the table: "simpipe.envs"
  """
  updateSimpipeEnvsByPk("sets the columns of the filtered rows to the given values" _set: SimpipeEnvsSetInput, pk_columns: SimpipeEnvsPkColumnsInput!): SimpipeEnvs
  """
  update multiples rows of table: "simpipe.envs"
  """
  updateSimpipeEnvsMany("updates to execute, in order" updates: [SimpipeEnvsUpdates!]!): [SimpipeEnvsMutationResponse]
  """
  update data of the table: "simpipe.logs"
  """
  updateSimpipeLogs("sets the columns of the filtered rows to the given values" _set: SimpipeLogsSetInput, "filter the rows which have to be updated" where: SimpipeLogsBoolExp!): SimpipeLogsMutationResponse
  """
  update single row of the table: "simpipe.logs"
  """
  updateSimpipeLogsByPk("sets the columns of the filtered rows to the given values" _set: SimpipeLogsSetInput, pk_columns: SimpipeLogsPkColumnsInput!): SimpipeLogs
  """
  update multiples rows of table: "simpipe.logs"
  """
  updateSimpipeLogsMany("updates to execute, in order" updates: [SimpipeLogsUpdates!]!): [SimpipeLogsMutationResponse]
  """
  update data of the table: "simpipe.run_status"
  """
  updateSimpipeRunStatus("sets the columns of the filtered rows to the given values" _set: SimpipeRunStatusSetInput, "filter the rows which have to be updated" where: SimpipeRunStatusBoolExp!): SimpipeRunStatusMutationResponse
  """
  update single row of the table: "simpipe.run_status"
  """
  updateSimpipeRunStatusByPk("sets the columns of the filtered rows to the given values" _set: SimpipeRunStatusSetInput, pk_columns: SimpipeRunStatusPkColumnsInput!): SimpipeRunStatus
  """
  update multiples rows of table: "simpipe.run_status"
  """
  updateSimpipeRunStatusMany("updates to execute, in order" updates: [SimpipeRunStatusUpdates!]!): [SimpipeRunStatusMutationResponse]
  """
  update data of the table: "simpipe.step_status"
  """
  updateSimpipeStepStatus("sets the columns of the filtered rows to the given values" _set: SimpipeStepStatusSetInput, "filter the rows which have to be updated" where: SimpipeStepStatusBoolExp!): SimpipeStepStatusMutationResponse
  """
  update single row of the table: "simpipe.step_status"
  """
  updateSimpipeStepStatusByPk("sets the columns of the filtered rows to the given values" _set: SimpipeStepStatusSetInput, pk_columns: SimpipeStepStatusPkColumnsInput!): SimpipeStepStatus
  """
  update multiples rows of table: "simpipe.step_status"
  """
  updateSimpipeStepStatusMany("updates to execute, in order" updates: [SimpipeStepStatusUpdates!]!): [SimpipeStepStatusMutationResponse]
  """
  update single row of the table: "simpipe.simulations"
  """
  updateSimulation("append existing jsonb value of filtered columns with new jsonb value" _append: simulationsAppendInput, "delete the field or element with specified path (for JSON arrays, negative integers count from the end)" _deleteAtPath: simulationsDeleteAtPathInput, "delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array" _deleteElem: simulationsDeleteElemInput, "delete key/value pair or string element. key/value pairs are matched based on their key value" _deleteKey: simulationsDeleteKeyInput, "prepend existing jsonb value of filtered columns with new jsonb value" _prepend: simulationsPrependInput, "sets the columns of the filtered rows to the given values" _set: simulationsSetInput, pk_columns: simulationsPkColumnsInput!): simulations
  """
  update data of the table: "simpipe.simulations"
  """
  updateSimulations("append existing jsonb value of filtered columns with new jsonb value" _append: simulationsAppendInput, "delete the field or element with specified path (for JSON arrays, negative integers count from the end)" _deleteAtPath: simulationsDeleteAtPathInput, "delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array" _deleteElem: simulationsDeleteElemInput, "delete key/value pair or string element. key/value pairs are matched based on their key value" _deleteKey: simulationsDeleteKeyInput, "prepend existing jsonb value of filtered columns with new jsonb value" _prepend: simulationsPrependInput, "sets the columns of the filtered rows to the given values" _set: simulationsSetInput, "filter the rows which have to be updated" where: simulationsBoolExp!): simulationsMutationResponse
  """
  update multiples rows of table: "simpipe.simulations"
  """
  updateSimulationsMany("updates to execute, in order" updates: [simulationsUpdates!]!): [simulationsMutationResponse]
  """
  update data of the table: "simpipe.steps"
  """
  updateSteps("increments the numeric columns with given value of the filtered values" _inc: stepsIncInput, "sets the columns of the filtered rows to the given values" _set: stepsSetInput, "filter the rows which have to be updated" where: stepsBoolExp!): stepsMutationResponse
  """
  update single row of the table: "simpipe.steps"
  """
  updateStepsByPk("increments the numeric columns with given value of the filtered values" _inc: stepsIncInput, "sets the columns of the filtered rows to the given values" _set: stepsSetInput, pk_columns: stepsPkColumnsInput!): steps
  """
  update multiples rows of table: "simpipe.steps"
  """
  updateStepsMany("updates to execute, in order" updates: [stepsUpdates!]!): [stepsMutationResponse]
}
"""
columns and relationships of "simpipe.network_received_bytes"
"""
type networkReceivedBytes {
  "An object relationship"
  run: runs
  runId: String
  "An object relationship"
  simulation: simulations
  simulationId: String
  "An object relationship"
  step: steps
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
aggregated selection of "simpipe.network_received_bytes"
"""
type networkReceivedBytesAggregate {
  aggregate: networkReceivedBytesAggregateFields
  nodes: [networkReceivedBytes!]!
}
"""
aggregate fields of "simpipe.network_received_bytes"
"""
type networkReceivedBytesAggregateFields {
  avg: networkReceivedBytesAvgFields
  count(columns: [networkReceivedBytesSelectColumn!], distinct: Boolean): Int!
  max: networkReceivedBytesMaxFields
  min: networkReceivedBytesMinFields
  stddev: networkReceivedBytesStddevFields
  stddevPop: networkReceivedBytesStddev_popFields
  stddevSamp: networkReceivedBytesStddev_sampFields
  sum: networkReceivedBytesSumFields
  varPop: networkReceivedBytesVar_popFields
  varSamp: networkReceivedBytesVar_sampFields
  variance: networkReceivedBytesVarianceFields
}
"""
order by aggregate values of table "simpipe.network_received_bytes"
"""
input networkReceivedBytesAggregateOrderBy {
  avg: networkReceivedBytes_avg_order_by
  count: OrderBy
  max: networkReceivedBytes_max_order_by
  min: networkReceivedBytes_min_order_by
  stddev: networkReceivedBytes_stddev_order_by
  stddev_pop: networkReceivedBytes_stddev_pop_order_by
  stddev_samp: networkReceivedBytes_stddev_samp_order_by
  sum: networkReceivedBytes_sum_order_by
  var_pop: networkReceivedBytes_var_pop_order_by
  var_samp: networkReceivedBytes_var_samp_order_by
  variance: networkReceivedBytes_variance_order_by
}
"""
input type for inserting array relation for remote table "simpipe.network_received_bytes"
"""
input networkReceivedBytesArrRelInsertInput {
  data: [networkReceivedBytesInsertInput!]!
}
"aggregate avg on columns"
type networkReceivedBytesAvgFields {
  value: Float
}
"""
Boolean expression to filter rows from the table "simpipe.network_received_bytes". All fields are combined with a logical 'AND'.
"""
input networkReceivedBytesBoolExp {
  _and: [networkReceivedBytesBoolExp!]
  _not: networkReceivedBytesBoolExp
  _or: [networkReceivedBytesBoolExp!]
  run: runsBoolExp
  runId: StringComparisonExp
  simulation: simulationsBoolExp
  simulationId: StringComparisonExp
  step: stepsBoolExp
  stepId: StringComparisonExp
  time: TimestamptzComparisonExp
  userId: StringComparisonExp
  value: Float8ComparisonExp
}
"""
input type for inserting data into table "simpipe.network_received_bytes"
"""
input networkReceivedBytesInsertInput {
  run: runsObjRelInsertInput
  runId: String
  simulation: simulationsObjRelInsertInput
  simulationId: String
  step: stepsObjRelInsertInput
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate max on columns"
type networkReceivedBytesMaxFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate min on columns"
type networkReceivedBytesMinFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
Ordering options when selecting data from "simpipe.network_received_bytes".
"""
input networkReceivedBytesOrderBy {
  run: runsOrderBy
  runId: OrderBy
  simulation: simulationsOrderBy
  simulationId: OrderBy
  step: stepsOrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select columns of table "simpipe.network_received_bytes"
"""
enum networkReceivedBytesSelectColumn {
  "column name"
  runId
  "column name"
  simulationId
  "column name"
  stepId
  "column name"
  time
  "column name"
  userId
  "column name"
  value
}
"aggregate stddev on columns"
type networkReceivedBytesStddevFields {
  value: Float
}
"aggregate stddev_pop on columns"
type networkReceivedBytesStddev_popFields {
  value: Float
}
"aggregate stddev_samp on columns"
type networkReceivedBytesStddev_sampFields {
  value: Float
}
"""
Streaming cursor of the table "networkReceivedBytes"
"""
input networkReceivedBytesStreamCursorInput {
  "Stream column input with initial value"
  initialValue: networkReceivedBytesStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input networkReceivedBytesStreamCursorValueInput {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate sum on columns"
type networkReceivedBytesSumFields {
  value: float8
}
"aggregate var_pop on columns"
type networkReceivedBytesVar_popFields {
  value: Float
}
"aggregate var_samp on columns"
type networkReceivedBytesVar_sampFields {
  value: Float
}
"aggregate variance on columns"
type networkReceivedBytesVarianceFields {
  value: Float
}
input networkReceivedBytes_aggregate_bool_exp {
  avg: networkReceivedBytes_aggregate_bool_exp_avg
  corr: networkReceivedBytes_aggregate_bool_exp_corr
  count: networkReceivedBytes_aggregate_bool_exp_count
  covar_samp: networkReceivedBytes_aggregate_bool_exp_covar_samp
  max: networkReceivedBytes_aggregate_bool_exp_max
  min: networkReceivedBytes_aggregate_bool_exp_min
  stddev_samp: networkReceivedBytes_aggregate_bool_exp_stddev_samp
  sum: networkReceivedBytes_aggregate_bool_exp_sum
  var_samp: networkReceivedBytes_aggregate_bool_exp_var_samp
}
input networkReceivedBytes_aggregate_bool_exp_avg {
  arguments: networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_avg_arguments_columns!
  distinct: Boolean
  filter: networkReceivedBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkReceivedBytes_aggregate_bool_exp_corr {
  arguments: networkReceivedBytes_aggregate_bool_exp_corr_arguments!
  distinct: Boolean
  filter: networkReceivedBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkReceivedBytes_aggregate_bool_exp_corr_arguments {
  X: networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_corr_arguments_columns!
  Y: networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_corr_arguments_columns!
}
input networkReceivedBytes_aggregate_bool_exp_count {
  arguments: [networkReceivedBytesSelectColumn!]
  distinct: Boolean
  filter: networkReceivedBytesBoolExp
  predicate: IntComparisonExp!
}
input networkReceivedBytes_aggregate_bool_exp_covar_samp {
  arguments: networkReceivedBytes_aggregate_bool_exp_covar_samp_arguments!
  distinct: Boolean
  filter: networkReceivedBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkReceivedBytes_aggregate_bool_exp_covar_samp_arguments {
  X: networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_covar_samp_arguments_columns!
  Y: networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_covar_samp_arguments_columns!
}
input networkReceivedBytes_aggregate_bool_exp_max {
  arguments: networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_max_arguments_columns!
  distinct: Boolean
  filter: networkReceivedBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkReceivedBytes_aggregate_bool_exp_min {
  arguments: networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_min_arguments_columns!
  distinct: Boolean
  filter: networkReceivedBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkReceivedBytes_aggregate_bool_exp_stddev_samp {
  arguments: networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_stddev_samp_arguments_columns!
  distinct: Boolean
  filter: networkReceivedBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkReceivedBytes_aggregate_bool_exp_sum {
  arguments: networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_sum_arguments_columns!
  distinct: Boolean
  filter: networkReceivedBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkReceivedBytes_aggregate_bool_exp_var_samp {
  arguments: networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_var_samp_arguments_columns!
  distinct: Boolean
  filter: networkReceivedBytesBoolExp
  predicate: Float8ComparisonExp!
}
"""
order by avg() on columns of table "simpipe.network_received_bytes"
"""
input networkReceivedBytes_avg_order_by {
  value: OrderBy
}
"""
order by max() on columns of table "simpipe.network_received_bytes"
"""
input networkReceivedBytes_max_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
order by min() on columns of table "simpipe.network_received_bytes"
"""
input networkReceivedBytes_min_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select "networkReceivedBytes_aggregate_bool_exp_avg_arguments_columns" columns of table "simpipe.network_received_bytes"
"""
enum networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_avg_arguments_columns {
  "column name"
  value
}
"""
select "networkReceivedBytes_aggregate_bool_exp_corr_arguments_columns" columns of table "simpipe.network_received_bytes"
"""
enum networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_corr_arguments_columns {
  "column name"
  value
}
"""
select "networkReceivedBytes_aggregate_bool_exp_covar_samp_arguments_columns" columns of table "simpipe.network_received_bytes"
"""
enum networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_covar_samp_arguments_columns {
  "column name"
  value
}
"""
select "networkReceivedBytes_aggregate_bool_exp_max_arguments_columns" columns of table "simpipe.network_received_bytes"
"""
enum networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_max_arguments_columns {
  "column name"
  value
}
"""
select "networkReceivedBytes_aggregate_bool_exp_min_arguments_columns" columns of table "simpipe.network_received_bytes"
"""
enum networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_min_arguments_columns {
  "column name"
  value
}
"""
select "networkReceivedBytes_aggregate_bool_exp_stddev_samp_arguments_columns" columns of table "simpipe.network_received_bytes"
"""
enum networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_stddev_samp_arguments_columns {
  "column name"
  value
}
"""
select "networkReceivedBytes_aggregate_bool_exp_sum_arguments_columns" columns of table "simpipe.network_received_bytes"
"""
enum networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_sum_arguments_columns {
  "column name"
  value
}
"""
select "networkReceivedBytes_aggregate_bool_exp_var_samp_arguments_columns" columns of table "simpipe.network_received_bytes"
"""
enum networkReceivedBytes_select_column_networkReceivedBytes_aggregate_bool_exp_var_samp_arguments_columns {
  "column name"
  value
}
"""
order by stddev() on columns of table "simpipe.network_received_bytes"
"""
input networkReceivedBytes_stddev_order_by {
  value: OrderBy
}
"""
order by stddev_pop() on columns of table "simpipe.network_received_bytes"
"""
input networkReceivedBytes_stddev_pop_order_by {
  value: OrderBy
}
"""
order by stddev_samp() on columns of table "simpipe.network_received_bytes"
"""
input networkReceivedBytes_stddev_samp_order_by {
  value: OrderBy
}
"""
order by sum() on columns of table "simpipe.network_received_bytes"
"""
input networkReceivedBytes_sum_order_by {
  value: OrderBy
}
"""
order by var_pop() on columns of table "simpipe.network_received_bytes"
"""
input networkReceivedBytes_var_pop_order_by {
  value: OrderBy
}
"""
order by var_samp() on columns of table "simpipe.network_received_bytes"
"""
input networkReceivedBytes_var_samp_order_by {
  value: OrderBy
}
"""
order by variance() on columns of table "simpipe.network_received_bytes"
"""
input networkReceivedBytes_variance_order_by {
  value: OrderBy
}
"""
columns and relationships of "simpipe.network_transmit_bytes"
"""
type networkTransmitBytes {
  "An object relationship"
  run: runs
  runId: String
  "An object relationship"
  simulation: simulations
  simulationId: String
  "An object relationship"
  step: steps
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
aggregated selection of "simpipe.network_transmit_bytes"
"""
type networkTransmitBytesAggregate {
  aggregate: networkTransmitBytesAggregateFields
  nodes: [networkTransmitBytes!]!
}
"""
aggregate fields of "simpipe.network_transmit_bytes"
"""
type networkTransmitBytesAggregateFields {
  avg: networkTransmitBytesAvgFields
  count(columns: [networkTransmitBytesSelectColumn!], distinct: Boolean): Int!
  max: networkTransmitBytesMaxFields
  min: networkTransmitBytesMinFields
  stddev: networkTransmitBytesStddevFields
  stddevPop: networkTransmitBytesStddev_popFields
  stddevSamp: networkTransmitBytesStddev_sampFields
  sum: networkTransmitBytesSumFields
  varPop: networkTransmitBytesVar_popFields
  varSamp: networkTransmitBytesVar_sampFields
  variance: networkTransmitBytesVarianceFields
}
"""
order by aggregate values of table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytesAggregateOrderBy {
  avg: networkTransmitBytes_avg_order_by
  count: OrderBy
  max: networkTransmitBytes_max_order_by
  min: networkTransmitBytes_min_order_by
  stddev: networkTransmitBytes_stddev_order_by
  stddev_pop: networkTransmitBytes_stddev_pop_order_by
  stddev_samp: networkTransmitBytes_stddev_samp_order_by
  sum: networkTransmitBytes_sum_order_by
  var_pop: networkTransmitBytes_var_pop_order_by
  var_samp: networkTransmitBytes_var_samp_order_by
  variance: networkTransmitBytes_variance_order_by
}
"""
input type for inserting array relation for remote table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytesArrRelInsertInput {
  data: [networkTransmitBytesInsertInput!]!
}
"aggregate avg on columns"
type networkTransmitBytesAvgFields {
  value: Float
}
"""
Boolean expression to filter rows from the table "simpipe.network_transmit_bytes". All fields are combined with a logical 'AND'.
"""
input networkTransmitBytesBoolExp {
  _and: [networkTransmitBytesBoolExp!]
  _not: networkTransmitBytesBoolExp
  _or: [networkTransmitBytesBoolExp!]
  run: runsBoolExp
  runId: StringComparisonExp
  simulation: simulationsBoolExp
  simulationId: StringComparisonExp
  step: stepsBoolExp
  stepId: StringComparisonExp
  time: TimestamptzComparisonExp
  userId: StringComparisonExp
  value: Float8ComparisonExp
}
"""
input type for inserting data into table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytesInsertInput {
  run: runsObjRelInsertInput
  runId: String
  simulation: simulationsObjRelInsertInput
  simulationId: String
  step: stepsObjRelInsertInput
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate max on columns"
type networkTransmitBytesMaxFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate min on columns"
type networkTransmitBytesMinFields {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"""
Ordering options when selecting data from "simpipe.network_transmit_bytes".
"""
input networkTransmitBytesOrderBy {
  run: runsOrderBy
  runId: OrderBy
  simulation: simulationsOrderBy
  simulationId: OrderBy
  step: stepsOrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select columns of table "simpipe.network_transmit_bytes"
"""
enum networkTransmitBytesSelectColumn {
  "column name"
  runId
  "column name"
  simulationId
  "column name"
  stepId
  "column name"
  time
  "column name"
  userId
  "column name"
  value
}
"aggregate stddev on columns"
type networkTransmitBytesStddevFields {
  value: Float
}
"aggregate stddev_pop on columns"
type networkTransmitBytesStddev_popFields {
  value: Float
}
"aggregate stddev_samp on columns"
type networkTransmitBytesStddev_sampFields {
  value: Float
}
"""
Streaming cursor of the table "networkTransmitBytes"
"""
input networkTransmitBytesStreamCursorInput {
  "Stream column input with initial value"
  initialValue: networkTransmitBytesStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input networkTransmitBytesStreamCursorValueInput {
  runId: String
  simulationId: String
  stepId: String
  time: timestamptz
  userId: String
  value: float8
}
"aggregate sum on columns"
type networkTransmitBytesSumFields {
  value: float8
}
"aggregate var_pop on columns"
type networkTransmitBytesVar_popFields {
  value: Float
}
"aggregate var_samp on columns"
type networkTransmitBytesVar_sampFields {
  value: Float
}
"aggregate variance on columns"
type networkTransmitBytesVarianceFields {
  value: Float
}
input networkTransmitBytes_aggregate_bool_exp {
  avg: networkTransmitBytes_aggregate_bool_exp_avg
  corr: networkTransmitBytes_aggregate_bool_exp_corr
  count: networkTransmitBytes_aggregate_bool_exp_count
  covar_samp: networkTransmitBytes_aggregate_bool_exp_covar_samp
  max: networkTransmitBytes_aggregate_bool_exp_max
  min: networkTransmitBytes_aggregate_bool_exp_min
  stddev_samp: networkTransmitBytes_aggregate_bool_exp_stddev_samp
  sum: networkTransmitBytes_aggregate_bool_exp_sum
  var_samp: networkTransmitBytes_aggregate_bool_exp_var_samp
}
input networkTransmitBytes_aggregate_bool_exp_avg {
  arguments: networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_avg_arguments_columns!
  distinct: Boolean
  filter: networkTransmitBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkTransmitBytes_aggregate_bool_exp_corr {
  arguments: networkTransmitBytes_aggregate_bool_exp_corr_arguments!
  distinct: Boolean
  filter: networkTransmitBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkTransmitBytes_aggregate_bool_exp_corr_arguments {
  X: networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_corr_arguments_columns!
  Y: networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_corr_arguments_columns!
}
input networkTransmitBytes_aggregate_bool_exp_count {
  arguments: [networkTransmitBytesSelectColumn!]
  distinct: Boolean
  filter: networkTransmitBytesBoolExp
  predicate: IntComparisonExp!
}
input networkTransmitBytes_aggregate_bool_exp_covar_samp {
  arguments: networkTransmitBytes_aggregate_bool_exp_covar_samp_arguments!
  distinct: Boolean
  filter: networkTransmitBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkTransmitBytes_aggregate_bool_exp_covar_samp_arguments {
  X: networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_covar_samp_arguments_columns!
  Y: networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_covar_samp_arguments_columns!
}
input networkTransmitBytes_aggregate_bool_exp_max {
  arguments: networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_max_arguments_columns!
  distinct: Boolean
  filter: networkTransmitBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkTransmitBytes_aggregate_bool_exp_min {
  arguments: networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_min_arguments_columns!
  distinct: Boolean
  filter: networkTransmitBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkTransmitBytes_aggregate_bool_exp_stddev_samp {
  arguments: networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_stddev_samp_arguments_columns!
  distinct: Boolean
  filter: networkTransmitBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkTransmitBytes_aggregate_bool_exp_sum {
  arguments: networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_sum_arguments_columns!
  distinct: Boolean
  filter: networkTransmitBytesBoolExp
  predicate: Float8ComparisonExp!
}
input networkTransmitBytes_aggregate_bool_exp_var_samp {
  arguments: networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_var_samp_arguments_columns!
  distinct: Boolean
  filter: networkTransmitBytesBoolExp
  predicate: Float8ComparisonExp!
}
"""
order by avg() on columns of table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytes_avg_order_by {
  value: OrderBy
}
"""
order by max() on columns of table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytes_max_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
order by min() on columns of table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytes_min_order_by {
  runId: OrderBy
  simulationId: OrderBy
  stepId: OrderBy
  time: OrderBy
  userId: OrderBy
  value: OrderBy
}
"""
select "networkTransmitBytes_aggregate_bool_exp_avg_arguments_columns" columns of table "simpipe.network_transmit_bytes"
"""
enum networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_avg_arguments_columns {
  "column name"
  value
}
"""
select "networkTransmitBytes_aggregate_bool_exp_corr_arguments_columns" columns of table "simpipe.network_transmit_bytes"
"""
enum networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_corr_arguments_columns {
  "column name"
  value
}
"""
select "networkTransmitBytes_aggregate_bool_exp_covar_samp_arguments_columns" columns of table "simpipe.network_transmit_bytes"
"""
enum networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_covar_samp_arguments_columns {
  "column name"
  value
}
"""
select "networkTransmitBytes_aggregate_bool_exp_max_arguments_columns" columns of table "simpipe.network_transmit_bytes"
"""
enum networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_max_arguments_columns {
  "column name"
  value
}
"""
select "networkTransmitBytes_aggregate_bool_exp_min_arguments_columns" columns of table "simpipe.network_transmit_bytes"
"""
enum networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_min_arguments_columns {
  "column name"
  value
}
"""
select "networkTransmitBytes_aggregate_bool_exp_stddev_samp_arguments_columns" columns of table "simpipe.network_transmit_bytes"
"""
enum networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_stddev_samp_arguments_columns {
  "column name"
  value
}
"""
select "networkTransmitBytes_aggregate_bool_exp_sum_arguments_columns" columns of table "simpipe.network_transmit_bytes"
"""
enum networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_sum_arguments_columns {
  "column name"
  value
}
"""
select "networkTransmitBytes_aggregate_bool_exp_var_samp_arguments_columns" columns of table "simpipe.network_transmit_bytes"
"""
enum networkTransmitBytes_select_column_networkTransmitBytes_aggregate_bool_exp_var_samp_arguments_columns {
  "column name"
  value
}
"""
order by stddev() on columns of table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytes_stddev_order_by {
  value: OrderBy
}
"""
order by stddev_pop() on columns of table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytes_stddev_pop_order_by {
  value: OrderBy
}
"""
order by stddev_samp() on columns of table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytes_stddev_samp_order_by {
  value: OrderBy
}
"""
order by sum() on columns of table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytes_sum_order_by {
  value: OrderBy
}
"""
order by var_pop() on columns of table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytes_var_pop_order_by {
  value: OrderBy
}
"""
order by var_samp() on columns of table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytes_var_samp_order_by {
  value: OrderBy
}
"""
order by variance() on columns of table "simpipe.network_transmit_bytes"
"""
input networkTransmitBytes_variance_order_by {
  value: OrderBy
}
type query_root {
  " Compute a presigned URL for uploading a file using a HTTP PUT. "
  computeUploadPresignedUrl: String!
  "An array relationship"
  cpu("distinct select on columns" distinctOn: [cpuSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [cpuOrderBy!], "filter the rows returned" where: cpuBoolExp): [cpu!]!
  "An aggregate relationship"
  cpuAggregate("distinct select on columns" distinctOn: [cpuSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [cpuOrderBy!], "filter the rows returned" where: cpuBoolExp): cpuAggregate!
  "An array relationship"
  fsReadsMerged("distinct select on columns" distinctOn: [fsReadsMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsReadsMergedOrderBy!], "filter the rows returned" where: fsReadsMergedBoolExp): [fsReadsMerged!]!
  "An aggregate relationship"
  fsReadsMergedAggregate("distinct select on columns" distinctOn: [fsReadsMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsReadsMergedOrderBy!], "filter the rows returned" where: fsReadsMergedBoolExp): fsReadsMergedAggregate!
  "An array relationship"
  fsWritesMerged("distinct select on columns" distinctOn: [fsWritesMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsWritesMergedOrderBy!], "filter the rows returned" where: fsWritesMergedBoolExp): [fsWritesMerged!]!
  "An aggregate relationship"
  fsWritesMergedAggregate("distinct select on columns" distinctOn: [fsWritesMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsWritesMergedOrderBy!], "filter the rows returned" where: fsWritesMergedBoolExp): fsWritesMergedAggregate!
  "An array relationship"
  memoryMaxUsageBytes("distinct select on columns" distinctOn: [memoryMaxUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryMaxUsageBytesOrderBy!], "filter the rows returned" where: memoryMaxUsageBytesBoolExp): [memoryMaxUsageBytes!]!
  "An aggregate relationship"
  memoryMaxUsageBytesAggregate("distinct select on columns" distinctOn: [memoryMaxUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryMaxUsageBytesOrderBy!], "filter the rows returned" where: memoryMaxUsageBytesBoolExp): memoryMaxUsageBytesAggregate!
  "An array relationship"
  memoryUsageBytes("distinct select on columns" distinctOn: [memoryUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryUsageBytesOrderBy!], "filter the rows returned" where: memoryUsageBytesBoolExp): [memoryUsageBytes!]!
  "An aggregate relationship"
  memoryUsageBytesAggregate("distinct select on columns" distinctOn: [memoryUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryUsageBytesOrderBy!], "filter the rows returned" where: memoryUsageBytesBoolExp): memoryUsageBytesAggregate!
  "An array relationship"
  networkReceivedBytes("distinct select on columns" distinctOn: [networkReceivedBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkReceivedBytesOrderBy!], "filter the rows returned" where: networkReceivedBytesBoolExp): [networkReceivedBytes!]!
  "An aggregate relationship"
  networkReceivedBytesAggregate("distinct select on columns" distinctOn: [networkReceivedBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkReceivedBytesOrderBy!], "filter the rows returned" where: networkReceivedBytesBoolExp): networkReceivedBytesAggregate!
  "An array relationship"
  networkTransmitBytes("distinct select on columns" distinctOn: [networkTransmitBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkTransmitBytesOrderBy!], "filter the rows returned" where: networkTransmitBytesBoolExp): [networkTransmitBytes!]!
  "An aggregate relationship"
  networkTransmitBytesAggregate("distinct select on columns" distinctOn: [networkTransmitBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkTransmitBytesOrderBy!], "filter the rows returned" where: networkTransmitBytesBoolExp): networkTransmitBytesAggregate!
  " Returns pong if the server is up and running. "
  ping: String!
  """
  fetch data from the table: "simpipe.runs" using primary key columns
  """
  run("UUID of the run, random by default" runId: uuid!): runs
  "An array relationship"
  runs("distinct select on columns" distinctOn: [runsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [runsOrderBy!], "filter the rows returned" where: runsBoolExp): [runs!]!
  "An aggregate relationship"
  runsAggregate("distinct select on columns" distinctOn: [runsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [runsOrderBy!], "filter the rows returned" where: runsBoolExp): runsAggregate!
  """
  fetch data from the table: "simpipe.envs"
  """
  simpipeEnvs("distinct select on columns" distinctOn: [SimpipeEnvsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeEnvsOrderBy!], "filter the rows returned" where: SimpipeEnvsBoolExp): [SimpipeEnvs!]!
  """
  fetch aggregated fields from the table: "simpipe.envs"
  """
  simpipeEnvsAggregate("distinct select on columns" distinctOn: [SimpipeEnvsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeEnvsOrderBy!], "filter the rows returned" where: SimpipeEnvsBoolExp): SimpipeEnvsAggregate!
  """
  fetch data from the table: "simpipe.envs" using primary key columns
  """
  simpipeEnvsByPk("UUID of the env, random by default" envId: uuid!): SimpipeEnvs
  """
  fetch data from the table: "simpipe.logs"
  """
  simpipeLogs("distinct select on columns" distinctOn: [SimpipeLogsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeLogsOrderBy!], "filter the rows returned" where: SimpipeLogsBoolExp): [SimpipeLogs!]!
  """
  fetch aggregated fields from the table: "simpipe.logs"
  """
  simpipeLogsAggregate("distinct select on columns" distinctOn: [SimpipeLogsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeLogsOrderBy!], "filter the rows returned" where: SimpipeLogsBoolExp): SimpipeLogsAggregate!
  """
  fetch data from the table: "simpipe.logs" using primary key columns
  """
  simpipeLogsByPk("UUID of the step, must exist in the steps table" stepId: uuid!): SimpipeLogs
  """
  fetch data from the table: "simpipe.run_status"
  """
  simpipeRunStatus("distinct select on columns" distinctOn: [SimpipeRunStatusSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeRunStatusOrderBy!], "filter the rows returned" where: SimpipeRunStatusBoolExp): [SimpipeRunStatus!]!
  """
  fetch aggregated fields from the table: "simpipe.run_status"
  """
  simpipeRunStatusAggregate("distinct select on columns" distinctOn: [SimpipeRunStatusSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeRunStatusOrderBy!], "filter the rows returned" where: SimpipeRunStatusBoolExp): SimpipeRunStatusAggregate!
  """
  fetch data from the table: "simpipe.run_status" using primary key columns
  """
  simpipeRunStatusByPk(value: String!): SimpipeRunStatus
  """
  fetch data from the table: "simpipe.step_status"
  """
  simpipeStepStatus("distinct select on columns" distinctOn: [SimpipeStepStatusSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeStepStatusOrderBy!], "filter the rows returned" where: SimpipeStepStatusBoolExp): [SimpipeStepStatus!]!
  """
  fetch aggregated fields from the table: "simpipe.step_status"
  """
  simpipeStepStatusAggregate("distinct select on columns" distinctOn: [SimpipeStepStatusSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeStepStatusOrderBy!], "filter the rows returned" where: SimpipeStepStatusBoolExp): SimpipeStepStatusAggregate!
  """
  fetch data from the table: "simpipe.step_status" using primary key columns
  """
  simpipeStepStatusByPk(value: String!): SimpipeStepStatus
  """
  fetch data from the table: "simpipe.simulations" using primary key columns
  """
  simulation("UUID of the simulation, random by default" simulationId: uuid!): simulations
  """
  fetch data from the table: "simpipe.simulations"
  """
  simulations("distinct select on columns" distinctOn: [simulationsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [simulationsOrderBy!], "filter the rows returned" where: simulationsBoolExp): [simulations!]!
  """
  fetch aggregated fields from the table: "simpipe.simulations"
  """
  simulationsAggregate("distinct select on columns" distinctOn: [simulationsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [simulationsOrderBy!], "filter the rows returned" where: simulationsBoolExp): simulationsAggregate!
  "An array relationship"
  steps("distinct select on columns" distinctOn: [stepsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [stepsOrderBy!], "filter the rows returned" where: stepsBoolExp): [steps!]!
  "An aggregate relationship"
  stepsAggregate("distinct select on columns" distinctOn: [stepsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [stepsOrderBy!], "filter the rows returned" where: stepsBoolExp): stepsAggregate!
  """
  fetch data from the table: "simpipe.steps" using primary key columns
  """
  stepsByPk("UUID of the step, random by default" stepId: uuid!): steps
  " Fetch the current username. "
  username: String!
}
"Simulation run"
type runs {
  "An array relationship"
  cpu("distinct select on columns" distinctOn: [cpuSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [cpuOrderBy!], "filter the rows returned" where: cpuBoolExp): [cpu!]!
  "An aggregate relationship"
  cpuAggregate("distinct select on columns" distinctOn: [cpuSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [cpuOrderBy!], "filter the rows returned" where: cpuBoolExp): cpuAggregate!
  "DateTime of when the run was created"
  created: timestamp!
  "DateTime of when the run was ended, NULL if not ended"
  ended: timestamp
  "An array relationship"
  fsReadsMerged("distinct select on columns" distinctOn: [fsReadsMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsReadsMergedOrderBy!], "filter the rows returned" where: fsReadsMergedBoolExp): [fsReadsMerged!]!
  "An aggregate relationship"
  fsReadsMergedAggregate("distinct select on columns" distinctOn: [fsReadsMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsReadsMergedOrderBy!], "filter the rows returned" where: fsReadsMergedBoolExp): fsReadsMergedAggregate!
  "An array relationship"
  fsWritesMerged("distinct select on columns" distinctOn: [fsWritesMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsWritesMergedOrderBy!], "filter the rows returned" where: fsWritesMergedBoolExp): [fsWritesMerged!]!
  "An aggregate relationship"
  fsWritesMergedAggregate("distinct select on columns" distinctOn: [fsWritesMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsWritesMergedOrderBy!], "filter the rows returned" where: fsWritesMergedBoolExp): fsWritesMergedAggregate!
  "An array relationship"
  memoryMaxUsageBytes("distinct select on columns" distinctOn: [memoryMaxUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryMaxUsageBytesOrderBy!], "filter the rows returned" where: memoryMaxUsageBytesBoolExp): [memoryMaxUsageBytes!]!
  "An aggregate relationship"
  memoryMaxUsageBytesAggregate("distinct select on columns" distinctOn: [memoryMaxUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryMaxUsageBytesOrderBy!], "filter the rows returned" where: memoryMaxUsageBytesBoolExp): memoryMaxUsageBytesAggregate!
  "An array relationship"
  memoryUsageBytes("distinct select on columns" distinctOn: [memoryUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryUsageBytesOrderBy!], "filter the rows returned" where: memoryUsageBytesBoolExp): [memoryUsageBytes!]!
  "An aggregate relationship"
  memoryUsageBytesAggregate("distinct select on columns" distinctOn: [memoryUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryUsageBytesOrderBy!], "filter the rows returned" where: memoryUsageBytesBoolExp): memoryUsageBytesAggregate!
  "Name of the run, it is unique per simulation"
  name: String!
  "An array relationship"
  networkReceivedBytes("distinct select on columns" distinctOn: [networkReceivedBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkReceivedBytesOrderBy!], "filter the rows returned" where: networkReceivedBytesBoolExp): [networkReceivedBytes!]!
  "An aggregate relationship"
  networkReceivedBytesAggregate("distinct select on columns" distinctOn: [networkReceivedBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkReceivedBytesOrderBy!], "filter the rows returned" where: networkReceivedBytesBoolExp): networkReceivedBytesAggregate!
  "An array relationship"
  networkTransmitBytes("distinct select on columns" distinctOn: [networkTransmitBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkTransmitBytesOrderBy!], "filter the rows returned" where: networkTransmitBytesBoolExp): [networkTransmitBytes!]!
  "An aggregate relationship"
  networkTransmitBytesAggregate("distinct select on columns" distinctOn: [networkTransmitBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkTransmitBytesOrderBy!], "filter the rows returned" where: networkTransmitBytesBoolExp): networkTransmitBytesAggregate!
  "UUID of the run, random by default"
  runId: uuid!
  "An object relationship"
  simulation: simulations!
  "UUID of the simulation, must exist in the simulations table"
  simulationId: uuid!
  "DateTime of when the run was started, NULL if not started"
  started: timestamp
  "Status of the run, must be one of the values in the run_status enum"
  status: SimpipeRunStatusEnum!
  "An array relationship"
  steps("distinct select on columns" distinctOn: [stepsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [stepsOrderBy!], "filter the rows returned" where: stepsBoolExp): [steps!]!
  "An aggregate relationship"
  stepsAggregate("distinct select on columns" distinctOn: [stepsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [stepsOrderBy!], "filter the rows returned" where: stepsBoolExp): stepsAggregate!
}
"""
aggregated selection of "simpipe.runs"
"""
type runsAggregate {
  aggregate: runsAggregateFields
  nodes: [runs!]!
}
"""
aggregate fields of "simpipe.runs"
"""
type runsAggregateFields {
  count(columns: [runsSelectColumn!], distinct: Boolean): Int!
  max: runsMaxFields
  min: runsMinFields
}
"""
order by aggregate values of table "simpipe.runs"
"""
input runsAggregateOrderBy {
  count: OrderBy
  max: runs_max_order_by
  min: runs_min_order_by
}
"""
input type for inserting array relation for remote table "simpipe.runs"
"""
input runsArrRelInsertInput {
  data: [runsInsertInput!]!
  "upsert condition"
  onConflict: runsOnConflict
}
"""
Boolean expression to filter rows from the table "simpipe.runs". All fields are combined with a logical 'AND'.
"""
input runsBoolExp {
  _and: [runsBoolExp!]
  _not: runsBoolExp
  _or: [runsBoolExp!]
  cpu: cpuBoolExp
  cpu_aggregate: cpu_aggregate_bool_exp
  created: TimestampComparisonExp
  ended: TimestampComparisonExp
  fsReadsMerged: fsReadsMergedBoolExp
  fsReadsMerged_aggregate: fsReadsMerged_aggregate_bool_exp
  fsWritesMerged: fsWritesMergedBoolExp
  fsWritesMerged_aggregate: fsWritesMerged_aggregate_bool_exp
  memoryMaxUsageBytes: memoryMaxUsageBytesBoolExp
  memoryMaxUsageBytes_aggregate: memoryMaxUsageBytes_aggregate_bool_exp
  memoryUsageBytes: memoryUsageBytesBoolExp
  memoryUsageBytes_aggregate: memoryUsageBytes_aggregate_bool_exp
  name: StringComparisonExp
  networkReceivedBytes: networkReceivedBytesBoolExp
  networkReceivedBytes_aggregate: networkReceivedBytes_aggregate_bool_exp
  networkTransmitBytes: networkTransmitBytesBoolExp
  networkTransmitBytes_aggregate: networkTransmitBytes_aggregate_bool_exp
  runId: UuidComparisonExp
  simulation: simulationsBoolExp
  simulationId: UuidComparisonExp
  started: TimestampComparisonExp
  status: SimpipeRunStatusEnumComparisonExp
  steps: stepsBoolExp
  steps_aggregate: steps_aggregate_bool_exp
}
"""
unique or primary key constraints on table "simpipe.runs"
"""
enum runsConstraint {
  """
  unique or primary key constraint on columns "run_id"
  """
  runs_pkey
  """
  unique or primary key constraint on columns "name", "simulation_id"
  """
  unique_run_name_per_simulation
}
"""
input type for inserting data into table "simpipe.runs"
"""
input runsInsertInput {
  cpu: cpuArrRelInsertInput
  "DateTime of when the run was created"
  created: timestamp
  "DateTime of when the run was ended, NULL if not ended"
  ended: timestamp
  fsReadsMerged: fsReadsMergedArrRelInsertInput
  fsWritesMerged: fsWritesMergedArrRelInsertInput
  memoryMaxUsageBytes: memoryMaxUsageBytesArrRelInsertInput
  memoryUsageBytes: memoryUsageBytesArrRelInsertInput
  "Name of the run, it is unique per simulation"
  name: String
  networkReceivedBytes: networkReceivedBytesArrRelInsertInput
  networkTransmitBytes: networkTransmitBytesArrRelInsertInput
  "UUID of the run, random by default"
  runId: uuid
  simulation: simulationsObjRelInsertInput
  "UUID of the simulation, must exist in the simulations table"
  simulationId: uuid
  "DateTime of when the run was started, NULL if not started"
  started: timestamp
  "Status of the run, must be one of the values in the run_status enum"
  status: SimpipeRunStatusEnum
  steps: stepsArrRelInsertInput
}
"aggregate max on columns"
type runsMaxFields {
  "DateTime of when the run was created"
  created: timestamp
  "DateTime of when the run was ended, NULL if not ended"
  ended: timestamp
  "Name of the run, it is unique per simulation"
  name: String
  "UUID of the run, random by default"
  runId: uuid
  "UUID of the simulation, must exist in the simulations table"
  simulationId: uuid
  "DateTime of when the run was started, NULL if not started"
  started: timestamp
}
"aggregate min on columns"
type runsMinFields {
  "DateTime of when the run was created"
  created: timestamp
  "DateTime of when the run was ended, NULL if not ended"
  ended: timestamp
  "Name of the run, it is unique per simulation"
  name: String
  "UUID of the run, random by default"
  runId: uuid
  "UUID of the simulation, must exist in the simulations table"
  simulationId: uuid
  "DateTime of when the run was started, NULL if not started"
  started: timestamp
}
"""
response of any mutation on the table "simpipe.runs"
"""
type runsMutationResponse {
  "number of rows affected by the mutation"
  affected_rows: Int!
  "data from the rows affected by the mutation"
  returning: [runs!]!
}
"""
input type for inserting object relation for remote table "simpipe.runs"
"""
input runsObjRelInsertInput {
  data: runsInsertInput!
  "upsert condition"
  onConflict: runsOnConflict
}
"""
on_conflict condition type for table "simpipe.runs"
"""
input runsOnConflict {
  constraint: runsConstraint!
  update_columns: [runsUpdateColumn!]! = []
  where: runsBoolExp
}
"""
Ordering options when selecting data from "simpipe.runs".
"""
input runsOrderBy {
  cpuAggregate: cpuAggregateOrderBy
  created: OrderBy
  ended: OrderBy
  fsReadsMergedAggregate: fsReadsMergedAggregateOrderBy
  fsWritesMergedAggregate: fsWritesMergedAggregateOrderBy
  memoryMaxUsageBytesAggregate: memoryMaxUsageBytesAggregateOrderBy
  memoryUsageBytesAggregate: memoryUsageBytesAggregateOrderBy
  name: OrderBy
  networkReceivedBytesAggregate: networkReceivedBytesAggregateOrderBy
  networkTransmitBytesAggregate: networkTransmitBytesAggregateOrderBy
  runId: OrderBy
  simulation: simulationsOrderBy
  simulationId: OrderBy
  started: OrderBy
  status: OrderBy
  stepsAggregate: stepsAggregateOrderBy
}
"primary key columns input for table: simpipe.runs"
input runsPkColumnsInput {
  "UUID of the run, random by default"
  runId: uuid!
}
"""
select columns of table "simpipe.runs"
"""
enum runsSelectColumn {
  "column name"
  created
  "column name"
  ended
  "column name"
  name
  "column name"
  runId
  "column name"
  simulationId
  "column name"
  started
  "column name"
  status
}
"""
input type for updating data in table "simpipe.runs"
"""
input runsSetInput {
  "DateTime of when the run was created"
  created: timestamp
  "DateTime of when the run was ended, NULL if not ended"
  ended: timestamp
  "Name of the run, it is unique per simulation"
  name: String
  "UUID of the run, random by default"
  runId: uuid
  "UUID of the simulation, must exist in the simulations table"
  simulationId: uuid
  "DateTime of when the run was started, NULL if not started"
  started: timestamp
  "Status of the run, must be one of the values in the run_status enum"
  status: SimpipeRunStatusEnum
}
"""
Streaming cursor of the table "runs"
"""
input runsStreamCursorInput {
  "Stream column input with initial value"
  initialValue: runsStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input runsStreamCursorValueInput {
  "DateTime of when the run was created"
  created: timestamp
  "DateTime of when the run was ended, NULL if not ended"
  ended: timestamp
  "Name of the run, it is unique per simulation"
  name: String
  "UUID of the run, random by default"
  runId: uuid
  "UUID of the simulation, must exist in the simulations table"
  simulationId: uuid
  "DateTime of when the run was started, NULL if not started"
  started: timestamp
  "Status of the run, must be one of the values in the run_status enum"
  status: SimpipeRunStatusEnum
}
"""
update columns of table "simpipe.runs"
"""
enum runsUpdateColumn {
  "column name"
  created
  "column name"
  ended
  "column name"
  name
  "column name"
  runId
  "column name"
  simulationId
  "column name"
  started
  "column name"
  status
}
input runsUpdates {
  "sets the columns of the filtered rows to the given values"
  _set: runsSetInput
  "filter the rows which have to be updated"
  where: runsBoolExp!
}
input runs_aggregate_bool_exp {
  count: runs_aggregate_bool_exp_count
}
input runs_aggregate_bool_exp_count {
  arguments: [runsSelectColumn!]
  distinct: Boolean
  filter: runsBoolExp
  predicate: IntComparisonExp!
}
"""
order by max() on columns of table "simpipe.runs"
"""
input runs_max_order_by {
  "DateTime of when the run was created"
  created: OrderBy
  "DateTime of when the run was ended, NULL if not ended"
  ended: OrderBy
  "Name of the run, it is unique per simulation"
  name: OrderBy
  "UUID of the run, random by default"
  runId: OrderBy
  "UUID of the simulation, must exist in the simulations table"
  simulationId: OrderBy
  "DateTime of when the run was started, NULL if not started"
  started: OrderBy
}
"""
order by min() on columns of table "simpipe.runs"
"""
input runs_min_order_by {
  "DateTime of when the run was created"
  created: OrderBy
  "DateTime of when the run was ended, NULL if not ended"
  ended: OrderBy
  "Name of the run, it is unique per simulation"
  name: OrderBy
  "UUID of the run, random by default"
  runId: OrderBy
  "UUID of the simulation, must exist in the simulations table"
  simulationId: OrderBy
  "DateTime of when the run was started, NULL if not started"
  started: OrderBy
}
input simpipe_envs_aggregate_bool_exp {
  count: simpipe_envs_aggregate_bool_exp_count
}
input simpipe_envs_aggregate_bool_exp_count {
  arguments: [SimpipeEnvsSelectColumn!]
  distinct: Boolean
  filter: SimpipeEnvsBoolExp
  predicate: IntComparisonExp!
}
"""
order by max() on columns of table "simpipe.envs"
"""
input simpipe_envs_max_order_by {
  "UUID of the env, random by default"
  envId: OrderBy
  "Name of the env, it is unique per step"
  name: OrderBy
  "UUID of the step, must exist in the steps table"
  stepId: OrderBy
  "Value of the env"
  value: OrderBy
}
"""
order by min() on columns of table "simpipe.envs"
"""
input simpipe_envs_min_order_by {
  "UUID of the env, random by default"
  envId: OrderBy
  "Name of the env, it is unique per step"
  name: OrderBy
  "UUID of the step, must exist in the steps table"
  stepId: OrderBy
  "Value of the env"
  value: OrderBy
}
"Simulations"
type simulations {
  "An array relationship"
  cpu("distinct select on columns" distinctOn: [cpuSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [cpuOrderBy!], "filter the rows returned" where: cpuBoolExp): [cpu!]!
  "An aggregate relationship"
  cpuAggregate("distinct select on columns" distinctOn: [cpuSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [cpuOrderBy!], "filter the rows returned" where: cpuBoolExp): cpuAggregate!
  "DateTime of when the simulation was created"
  created: timestamp!
  "An array relationship"
  fsReadsMerged("distinct select on columns" distinctOn: [fsReadsMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsReadsMergedOrderBy!], "filter the rows returned" where: fsReadsMergedBoolExp): [fsReadsMerged!]!
  "An aggregate relationship"
  fsReadsMergedAggregate("distinct select on columns" distinctOn: [fsReadsMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsReadsMergedOrderBy!], "filter the rows returned" where: fsReadsMergedBoolExp): fsReadsMergedAggregate!
  "An array relationship"
  fsWritesMerged("distinct select on columns" distinctOn: [fsWritesMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsWritesMergedOrderBy!], "filter the rows returned" where: fsWritesMergedBoolExp): [fsWritesMerged!]!
  "An aggregate relationship"
  fsWritesMergedAggregate("distinct select on columns" distinctOn: [fsWritesMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsWritesMergedOrderBy!], "filter the rows returned" where: fsWritesMergedBoolExp): fsWritesMergedAggregate!
  "An array relationship"
  memoryMaxUsageBytes("distinct select on columns" distinctOn: [memoryMaxUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryMaxUsageBytesOrderBy!], "filter the rows returned" where: memoryMaxUsageBytesBoolExp): [memoryMaxUsageBytes!]!
  "An aggregate relationship"
  memoryMaxUsageBytesAggregate("distinct select on columns" distinctOn: [memoryMaxUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryMaxUsageBytesOrderBy!], "filter the rows returned" where: memoryMaxUsageBytesBoolExp): memoryMaxUsageBytesAggregate!
  "An array relationship"
  memoryUsageBytes("distinct select on columns" distinctOn: [memoryUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryUsageBytesOrderBy!], "filter the rows returned" where: memoryUsageBytesBoolExp): [memoryUsageBytes!]!
  "An aggregate relationship"
  memoryUsageBytesAggregate("distinct select on columns" distinctOn: [memoryUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryUsageBytesOrderBy!], "filter the rows returned" where: memoryUsageBytesBoolExp): memoryUsageBytesAggregate!
  "Name of the simulation, it is unique"
  name: String!
  "An array relationship"
  networkReceivedBytes("distinct select on columns" distinctOn: [networkReceivedBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkReceivedBytesOrderBy!], "filter the rows returned" where: networkReceivedBytesBoolExp): [networkReceivedBytes!]!
  "An aggregate relationship"
  networkReceivedBytesAggregate("distinct select on columns" distinctOn: [networkReceivedBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkReceivedBytesOrderBy!], "filter the rows returned" where: networkReceivedBytesBoolExp): networkReceivedBytesAggregate!
  "An array relationship"
  networkTransmitBytes("distinct select on columns" distinctOn: [networkTransmitBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkTransmitBytesOrderBy!], "filter the rows returned" where: networkTransmitBytesBoolExp): [networkTransmitBytes!]!
  "An aggregate relationship"
  networkTransmitBytesAggregate("distinct select on columns" distinctOn: [networkTransmitBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkTransmitBytesOrderBy!], "filter the rows returned" where: networkTransmitBytesBoolExp): networkTransmitBytesAggregate!
  "Description of the pipeline in JSON"
  pipelineDescription("JSON select path" path: String): jsonb!
  "An array relationship"
  runs("distinct select on columns" distinctOn: [runsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [runsOrderBy!], "filter the rows returned" where: runsBoolExp): [runs!]!
  "An aggregate relationship"
  runsAggregate("distinct select on columns" distinctOn: [runsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [runsOrderBy!], "filter the rows returned" where: runsBoolExp): runsAggregate!
  "UUID of the simulation, random by default"
  simulationId: uuid!
  "User ID of the owner of the simulation"
  userId: String!
}
"""
aggregated selection of "simpipe.simulations"
"""
type simulationsAggregate {
  aggregate: simulationsAggregateFields
  nodes: [simulations!]!
}
"""
aggregate fields of "simpipe.simulations"
"""
type simulationsAggregateFields {
  count(columns: [simulationsSelectColumn!], distinct: Boolean): Int!
  max: simulationsMaxFields
  min: simulationsMinFields
}
"append existing jsonb value of filtered columns with new jsonb value"
input simulationsAppendInput {
  "Description of the pipeline in JSON"
  pipelineDescription: jsonb
}
"""
Boolean expression to filter rows from the table "simpipe.simulations". All fields are combined with a logical 'AND'.
"""
input simulationsBoolExp {
  _and: [simulationsBoolExp!]
  _not: simulationsBoolExp
  _or: [simulationsBoolExp!]
  cpu: cpuBoolExp
  cpu_aggregate: cpu_aggregate_bool_exp
  created: TimestampComparisonExp
  fsReadsMerged: fsReadsMergedBoolExp
  fsReadsMerged_aggregate: fsReadsMerged_aggregate_bool_exp
  fsWritesMerged: fsWritesMergedBoolExp
  fsWritesMerged_aggregate: fsWritesMerged_aggregate_bool_exp
  memoryMaxUsageBytes: memoryMaxUsageBytesBoolExp
  memoryMaxUsageBytes_aggregate: memoryMaxUsageBytes_aggregate_bool_exp
  memoryUsageBytes: memoryUsageBytesBoolExp
  memoryUsageBytes_aggregate: memoryUsageBytes_aggregate_bool_exp
  name: StringComparisonExp
  networkReceivedBytes: networkReceivedBytesBoolExp
  networkReceivedBytes_aggregate: networkReceivedBytes_aggregate_bool_exp
  networkTransmitBytes: networkTransmitBytesBoolExp
  networkTransmitBytes_aggregate: networkTransmitBytes_aggregate_bool_exp
  pipelineDescription: JsonbComparisonExp
  runs: runsBoolExp
  runs_aggregate: runs_aggregate_bool_exp
  simulationId: UuidComparisonExp
  userId: StringComparisonExp
}
"""
unique or primary key constraints on table "simpipe.simulations"
"""
enum simulationsConstraint {
  """
  unique or primary key constraint on columns "name"
  """
  simulations_name_key
  """
  unique or primary key constraint on columns "simulation_id"
  """
  simulations_pkey
}
"delete the field or element with specified path (for JSON arrays, negative integers count from the end)"
input simulationsDeleteAtPathInput {
  "Description of the pipeline in JSON"
  pipelineDescription: [String!]
}
"delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array"
input simulationsDeleteElemInput {
  "Description of the pipeline in JSON"
  pipelineDescription: Int
}
"delete key/value pair or string element. key/value pairs are matched based on their key value"
input simulationsDeleteKeyInput {
  "Description of the pipeline in JSON"
  pipelineDescription: String
}
"""
input type for inserting data into table "simpipe.simulations"
"""
input simulationsInsertInput {
  cpu: cpuArrRelInsertInput
  "DateTime of when the simulation was created"
  created: timestamp
  fsReadsMerged: fsReadsMergedArrRelInsertInput
  fsWritesMerged: fsWritesMergedArrRelInsertInput
  memoryMaxUsageBytes: memoryMaxUsageBytesArrRelInsertInput
  memoryUsageBytes: memoryUsageBytesArrRelInsertInput
  "Name of the simulation, it is unique"
  name: String
  networkReceivedBytes: networkReceivedBytesArrRelInsertInput
  networkTransmitBytes: networkTransmitBytesArrRelInsertInput
  "Description of the pipeline in JSON"
  pipelineDescription: jsonb
  runs: runsArrRelInsertInput
  "UUID of the simulation, random by default"
  simulationId: uuid
  "User ID of the owner of the simulation"
  userId: String
}
"aggregate max on columns"
type simulationsMaxFields {
  "DateTime of when the simulation was created"
  created: timestamp
  "Name of the simulation, it is unique"
  name: String
  "UUID of the simulation, random by default"
  simulationId: uuid
  "User ID of the owner of the simulation"
  userId: String
}
"aggregate min on columns"
type simulationsMinFields {
  "DateTime of when the simulation was created"
  created: timestamp
  "Name of the simulation, it is unique"
  name: String
  "UUID of the simulation, random by default"
  simulationId: uuid
  "User ID of the owner of the simulation"
  userId: String
}
"""
response of any mutation on the table "simpipe.simulations"
"""
type simulationsMutationResponse {
  "number of rows affected by the mutation"
  affected_rows: Int!
  "data from the rows affected by the mutation"
  returning: [simulations!]!
}
"""
input type for inserting object relation for remote table "simpipe.simulations"
"""
input simulationsObjRelInsertInput {
  data: simulationsInsertInput!
  "upsert condition"
  onConflict: simulationsOnConflict
}
"""
on_conflict condition type for table "simpipe.simulations"
"""
input simulationsOnConflict {
  constraint: simulationsConstraint!
  update_columns: [simulationsUpdateColumn!]! = []
  where: simulationsBoolExp
}
"""
Ordering options when selecting data from "simpipe.simulations".
"""
input simulationsOrderBy {
  cpuAggregate: cpuAggregateOrderBy
  created: OrderBy
  fsReadsMergedAggregate: fsReadsMergedAggregateOrderBy
  fsWritesMergedAggregate: fsWritesMergedAggregateOrderBy
  memoryMaxUsageBytesAggregate: memoryMaxUsageBytesAggregateOrderBy
  memoryUsageBytesAggregate: memoryUsageBytesAggregateOrderBy
  name: OrderBy
  networkReceivedBytesAggregate: networkReceivedBytesAggregateOrderBy
  networkTransmitBytesAggregate: networkTransmitBytesAggregateOrderBy
  pipelineDescription: OrderBy
  runsAggregate: runsAggregateOrderBy
  simulationId: OrderBy
  userId: OrderBy
}
"primary key columns input for table: simpipe.simulations"
input simulationsPkColumnsInput {
  "UUID of the simulation, random by default"
  simulationId: uuid!
}
"prepend existing jsonb value of filtered columns with new jsonb value"
input simulationsPrependInput {
  "Description of the pipeline in JSON"
  pipelineDescription: jsonb
}
"""
select columns of table "simpipe.simulations"
"""
enum simulationsSelectColumn {
  "column name"
  created
  "column name"
  name
  "column name"
  pipelineDescription
  "column name"
  simulationId
  "column name"
  userId
}
"""
input type for updating data in table "simpipe.simulations"
"""
input simulationsSetInput {
  "DateTime of when the simulation was created"
  created: timestamp
  "Name of the simulation, it is unique"
  name: String
  "Description of the pipeline in JSON"
  pipelineDescription: jsonb
  "UUID of the simulation, random by default"
  simulationId: uuid
  "User ID of the owner of the simulation"
  userId: String
}
"""
Streaming cursor of the table "simulations"
"""
input simulationsStreamCursorInput {
  "Stream column input with initial value"
  initialValue: simulationsStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input simulationsStreamCursorValueInput {
  "DateTime of when the simulation was created"
  created: timestamp
  "Name of the simulation, it is unique"
  name: String
  "Description of the pipeline in JSON"
  pipelineDescription: jsonb
  "UUID of the simulation, random by default"
  simulationId: uuid
  "User ID of the owner of the simulation"
  userId: String
}
"""
update columns of table "simpipe.simulations"
"""
enum simulationsUpdateColumn {
  "column name"
  created
  "column name"
  name
  "column name"
  pipelineDescription
  "column name"
  simulationId
  "column name"
  userId
}
input simulationsUpdates {
  "append existing jsonb value of filtered columns with new jsonb value"
  _append: simulationsAppendInput
  "delete the field or element with specified path (for JSON arrays, negative integers count from the end)"
  _deleteAtPath: simulationsDeleteAtPathInput
  "delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array"
  _deleteElem: simulationsDeleteElemInput
  "delete key/value pair or string element. key/value pairs are matched based on their key value"
  _deleteKey: simulationsDeleteKeyInput
  "prepend existing jsonb value of filtered columns with new jsonb value"
  _prepend: simulationsPrependInput
  "sets the columns of the filtered rows to the given values"
  _set: simulationsSetInput
  "filter the rows which have to be updated"
  where: simulationsBoolExp!
}
"Steps in a run"
type steps {
  "An array relationship"
  cpu("distinct select on columns" distinctOn: [cpuSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [cpuOrderBy!], "filter the rows returned" where: cpuBoolExp): [cpu!]!
  "An aggregate relationship"
  cpuAggregate("distinct select on columns" distinctOn: [cpuSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [cpuOrderBy!], "filter the rows returned" where: cpuBoolExp): cpuAggregate!
  "DateTime of when the step was created"
  created: timestamp!
  "DateTime of when the step was ended, NULL if not ended"
  ended: timestamp
  "An array relationship"
  envs("distinct select on columns" distinctOn: [SimpipeEnvsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeEnvsOrderBy!], "filter the rows returned" where: SimpipeEnvsBoolExp): [SimpipeEnvs!]!
  "An aggregate relationship"
  envsAggregate("distinct select on columns" distinctOn: [SimpipeEnvsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeEnvsOrderBy!], "filter the rows returned" where: SimpipeEnvsBoolExp): SimpipeEnvsAggregate!
  "An array relationship"
  fsReadsMerged("distinct select on columns" distinctOn: [fsReadsMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsReadsMergedOrderBy!], "filter the rows returned" where: fsReadsMergedBoolExp): [fsReadsMerged!]!
  "An aggregate relationship"
  fsReadsMergedAggregate("distinct select on columns" distinctOn: [fsReadsMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsReadsMergedOrderBy!], "filter the rows returned" where: fsReadsMergedBoolExp): fsReadsMergedAggregate!
  "An array relationship"
  fsWritesMerged("distinct select on columns" distinctOn: [fsWritesMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsWritesMergedOrderBy!], "filter the rows returned" where: fsWritesMergedBoolExp): [fsWritesMerged!]!
  "An aggregate relationship"
  fsWritesMergedAggregate("distinct select on columns" distinctOn: [fsWritesMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsWritesMergedOrderBy!], "filter the rows returned" where: fsWritesMergedBoolExp): fsWritesMergedAggregate!
  "Docker image of the step"
  image: String!
  "An object relationship"
  log: SimpipeLogs
  "An array relationship"
  memoryMaxUsageBytes("distinct select on columns" distinctOn: [memoryMaxUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryMaxUsageBytesOrderBy!], "filter the rows returned" where: memoryMaxUsageBytesBoolExp): [memoryMaxUsageBytes!]!
  "An aggregate relationship"
  memoryMaxUsageBytesAggregate("distinct select on columns" distinctOn: [memoryMaxUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryMaxUsageBytesOrderBy!], "filter the rows returned" where: memoryMaxUsageBytesBoolExp): memoryMaxUsageBytesAggregate!
  "An array relationship"
  memoryUsageBytes("distinct select on columns" distinctOn: [stepsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [stepsOrderBy!], "filter the rows returned" where: stepsBoolExp): [steps!]!
  "An aggregate relationship"
  memoryUsageBytesAggregate("distinct select on columns" distinctOn: [stepsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [stepsOrderBy!], "filter the rows returned" where: stepsBoolExp): stepsAggregate!
  "Name of the step, it is unique per run"
  name: String!
  "An array relationship"
  networkReceivedBytes("distinct select on columns" distinctOn: [networkReceivedBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkReceivedBytesOrderBy!], "filter the rows returned" where: networkReceivedBytesBoolExp): [networkReceivedBytes!]!
  "An aggregate relationship"
  networkReceivedBytesAggregate("distinct select on columns" distinctOn: [networkReceivedBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkReceivedBytesOrderBy!], "filter the rows returned" where: networkReceivedBytesBoolExp): networkReceivedBytesAggregate!
  "An array relationship"
  networkTransmitBytes("distinct select on columns" distinctOn: [networkTransmitBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkTransmitBytesOrderBy!], "filter the rows returned" where: networkTransmitBytesBoolExp): [networkTransmitBytes!]!
  "An aggregate relationship"
  networkTransmitBytesAggregate("distinct select on columns" distinctOn: [networkTransmitBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkTransmitBytesOrderBy!], "filter the rows returned" where: networkTransmitBytesBoolExp): networkTransmitBytesAggregate!
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Int!
  "An object relationship"
  run: runs!
  "UUID of the run, must exist in the runs table"
  runId: uuid!
  "DateTime of when the step was started, NULL if not started"
  started: timestamp
  "Status of the step, must be one of the values in the step_status enum"
  status: SimpipeStepStatusEnum!
  "UUID of the step, random by default"
  stepId: uuid!
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Int!
}
"""
aggregated selection of "simpipe.steps"
"""
type stepsAggregate {
  aggregate: stepsAggregateFields
  nodes: [steps!]!
}
"""
aggregate fields of "simpipe.steps"
"""
type stepsAggregateFields {
  avg: stepsAvgFields
  count(columns: [stepsSelectColumn!], distinct: Boolean): Int!
  max: stepsMaxFields
  min: stepsMinFields
  stddev: stepsStddevFields
  stddevPop: stepsStddev_popFields
  stddevSamp: stepsStddev_sampFields
  sum: stepsSumFields
  varPop: stepsVar_popFields
  varSamp: stepsVar_sampFields
  variance: stepsVarianceFields
}
"""
order by aggregate values of table "simpipe.steps"
"""
input stepsAggregateOrderBy {
  avg: steps_avg_order_by
  count: OrderBy
  max: steps_max_order_by
  min: steps_min_order_by
  stddev: steps_stddev_order_by
  stddev_pop: steps_stddev_pop_order_by
  stddev_samp: steps_stddev_samp_order_by
  sum: steps_sum_order_by
  var_pop: steps_var_pop_order_by
  var_samp: steps_var_samp_order_by
  variance: steps_variance_order_by
}
"""
input type for inserting array relation for remote table "simpipe.steps"
"""
input stepsArrRelInsertInput {
  data: [stepsInsertInput!]!
  "upsert condition"
  onConflict: stepsOnConflict
}
"aggregate avg on columns"
type stepsAvgFields {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Float
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Float
}
"""
Boolean expression to filter rows from the table "simpipe.steps". All fields are combined with a logical 'AND'.
"""
input stepsBoolExp {
  _and: [stepsBoolExp!]
  _not: stepsBoolExp
  _or: [stepsBoolExp!]
  cpu: cpuBoolExp
  cpu_aggregate: cpu_aggregate_bool_exp
  created: TimestampComparisonExp
  ended: TimestampComparisonExp
  envs: SimpipeEnvsBoolExp
  envs_aggregate: simpipe_envs_aggregate_bool_exp
  fsReadsMerged: fsReadsMergedBoolExp
  fsReadsMerged_aggregate: fsReadsMerged_aggregate_bool_exp
  fsWritesMerged: fsWritesMergedBoolExp
  fsWritesMerged_aggregate: fsWritesMerged_aggregate_bool_exp
  image: StringComparisonExp
  log: SimpipeLogsBoolExp
  memoryMaxUsageBytes: memoryMaxUsageBytesBoolExp
  memoryMaxUsageBytes_aggregate: memoryMaxUsageBytes_aggregate_bool_exp
  memoryUsageBytes: stepsBoolExp
  memoryUsageBytes_aggregate: steps_aggregate_bool_exp
  name: StringComparisonExp
  networkReceivedBytes: networkReceivedBytesBoolExp
  networkReceivedBytes_aggregate: networkReceivedBytes_aggregate_bool_exp
  networkTransmitBytes: networkTransmitBytesBoolExp
  networkTransmitBytes_aggregate: networkTransmitBytes_aggregate_bool_exp
  pipelineStepNumber: IntComparisonExp
  run: runsBoolExp
  runId: UuidComparisonExp
  started: TimestampComparisonExp
  status: SimpipeStepStatusEnumComparisonExp
  stepId: UuidComparisonExp
  timeout: IntComparisonExp
}
"""
unique or primary key constraints on table "simpipe.steps"
"""
enum stepsConstraint {
  """
  unique or primary key constraint on columns "run_id", "pipeline_step_number"
  """
  steps_pipeline_step_number_run_id_key
  """
  unique or primary key constraint on columns "step_id"
  """
  steps_pkey
  """
  unique or primary key constraint on columns "run_id", "name"
  """
  steps_run_id_name_key
}
"""
input type for incrementing numeric columns in table "simpipe.steps"
"""
input stepsIncInput {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Int
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Int
}
"""
input type for inserting data into table "simpipe.steps"
"""
input stepsInsertInput {
  cpu: cpuArrRelInsertInput
  "DateTime of when the step was created"
  created: timestamp
  "DateTime of when the step was ended, NULL if not ended"
  ended: timestamp
  envs: SimpipeEnvsArrRelInsertInput
  fsReadsMerged: fsReadsMergedArrRelInsertInput
  fsWritesMerged: fsWritesMergedArrRelInsertInput
  "Docker image of the step"
  image: String
  log: SimpipeLogsObjRelInsertInput
  memoryMaxUsageBytes: memoryMaxUsageBytesArrRelInsertInput
  memoryUsageBytes: stepsArrRelInsertInput
  "Name of the step, it is unique per run"
  name: String
  networkReceivedBytes: networkReceivedBytesArrRelInsertInput
  networkTransmitBytes: networkTransmitBytesArrRelInsertInput
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Int
  run: runsObjRelInsertInput
  "UUID of the run, must exist in the runs table"
  runId: uuid
  "DateTime of when the step was started, NULL if not started"
  started: timestamp
  "Status of the step, must be one of the values in the step_status enum"
  status: SimpipeStepStatusEnum
  "UUID of the step, random by default"
  stepId: uuid
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Int
}
"aggregate max on columns"
type stepsMaxFields {
  "DateTime of when the step was created"
  created: timestamp
  "DateTime of when the step was ended, NULL if not ended"
  ended: timestamp
  "Docker image of the step"
  image: String
  "Name of the step, it is unique per run"
  name: String
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Int
  "UUID of the run, must exist in the runs table"
  runId: uuid
  "DateTime of when the step was started, NULL if not started"
  started: timestamp
  "UUID of the step, random by default"
  stepId: uuid
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Int
}
"aggregate min on columns"
type stepsMinFields {
  "DateTime of when the step was created"
  created: timestamp
  "DateTime of when the step was ended, NULL if not ended"
  ended: timestamp
  "Docker image of the step"
  image: String
  "Name of the step, it is unique per run"
  name: String
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Int
  "UUID of the run, must exist in the runs table"
  runId: uuid
  "DateTime of when the step was started, NULL if not started"
  started: timestamp
  "UUID of the step, random by default"
  stepId: uuid
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Int
}
"""
response of any mutation on the table "simpipe.steps"
"""
type stepsMutationResponse {
  "number of rows affected by the mutation"
  affected_rows: Int!
  "data from the rows affected by the mutation"
  returning: [steps!]!
}
"""
input type for inserting object relation for remote table "simpipe.steps"
"""
input stepsObjRelInsertInput {
  data: stepsInsertInput!
  "upsert condition"
  onConflict: stepsOnConflict
}
"""
on_conflict condition type for table "simpipe.steps"
"""
input stepsOnConflict {
  constraint: stepsConstraint!
  update_columns: [stepsUpdateColumn!]! = []
  where: stepsBoolExp
}
"""
Ordering options when selecting data from "simpipe.steps".
"""
input stepsOrderBy {
  cpuAggregate: cpuAggregateOrderBy
  created: OrderBy
  ended: OrderBy
  envsAggregate: SimpipeEnvsAggregateOrderBy
  fsReadsMergedAggregate: fsReadsMergedAggregateOrderBy
  fsWritesMergedAggregate: fsWritesMergedAggregateOrderBy
  image: OrderBy
  log: SimpipeLogsOrderBy
  memoryMaxUsageBytesAggregate: memoryMaxUsageBytesAggregateOrderBy
  memoryUsageBytesAggregate: stepsAggregateOrderBy
  name: OrderBy
  networkReceivedBytesAggregate: networkReceivedBytesAggregateOrderBy
  networkTransmitBytesAggregate: networkTransmitBytesAggregateOrderBy
  pipelineStepNumber: OrderBy
  run: runsOrderBy
  runId: OrderBy
  started: OrderBy
  status: OrderBy
  stepId: OrderBy
  timeout: OrderBy
}
"primary key columns input for table: simpipe.steps"
input stepsPkColumnsInput {
  "UUID of the step, random by default"
  stepId: uuid!
}
"""
select columns of table "simpipe.steps"
"""
enum stepsSelectColumn {
  "column name"
  created
  "column name"
  ended
  "column name"
  image
  "column name"
  name
  "column name"
  pipelineStepNumber
  "column name"
  runId
  "column name"
  started
  "column name"
  status
  "column name"
  stepId
  "column name"
  timeout
}
"""
input type for updating data in table "simpipe.steps"
"""
input stepsSetInput {
  "DateTime of when the step was created"
  created: timestamp
  "DateTime of when the step was ended, NULL if not ended"
  ended: timestamp
  "Docker image of the step"
  image: String
  "Name of the step, it is unique per run"
  name: String
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Int
  "UUID of the run, must exist in the runs table"
  runId: uuid
  "DateTime of when the step was started, NULL if not started"
  started: timestamp
  "Status of the step, must be one of the values in the step_status enum"
  status: SimpipeStepStatusEnum
  "UUID of the step, random by default"
  stepId: uuid
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Int
}
"aggregate stddev on columns"
type stepsStddevFields {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Float
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Float
}
"aggregate stddev_pop on columns"
type stepsStddev_popFields {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Float
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Float
}
"aggregate stddev_samp on columns"
type stepsStddev_sampFields {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Float
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Float
}
"""
Streaming cursor of the table "steps"
"""
input stepsStreamCursorInput {
  "Stream column input with initial value"
  initialValue: stepsStreamCursorValueInput!
  "cursor ordering"
  ordering: CursorOrdering
}
"Initial value of the column from where the streaming should start"
input stepsStreamCursorValueInput {
  "DateTime of when the step was created"
  created: timestamp
  "DateTime of when the step was ended, NULL if not ended"
  ended: timestamp
  "Docker image of the step"
  image: String
  "Name of the step, it is unique per run"
  name: String
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Int
  "UUID of the run, must exist in the runs table"
  runId: uuid
  "DateTime of when the step was started, NULL if not started"
  started: timestamp
  "Status of the step, must be one of the values in the step_status enum"
  status: SimpipeStepStatusEnum
  "UUID of the step, random by default"
  stepId: uuid
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Int
}
"aggregate sum on columns"
type stepsSumFields {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Int
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Int
}
"""
update columns of table "simpipe.steps"
"""
enum stepsUpdateColumn {
  "column name"
  created
  "column name"
  ended
  "column name"
  image
  "column name"
  name
  "column name"
  pipelineStepNumber
  "column name"
  runId
  "column name"
  started
  "column name"
  status
  "column name"
  stepId
  "column name"
  timeout
}
input stepsUpdates {
  "increments the numeric columns with given value of the filtered values"
  _inc: stepsIncInput
  "sets the columns of the filtered rows to the given values"
  _set: stepsSetInput
  "filter the rows which have to be updated"
  where: stepsBoolExp!
}
"aggregate var_pop on columns"
type stepsVar_popFields {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Float
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Float
}
"aggregate var_samp on columns"
type stepsVar_sampFields {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Float
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Float
}
"aggregate variance on columns"
type stepsVarianceFields {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: Float
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: Float
}
input steps_aggregate_bool_exp {
  count: steps_aggregate_bool_exp_count
}
input steps_aggregate_bool_exp_count {
  arguments: [stepsSelectColumn!]
  distinct: Boolean
  filter: stepsBoolExp
  predicate: IntComparisonExp!
}
"""
order by avg() on columns of table "simpipe.steps"
"""
input steps_avg_order_by {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: OrderBy
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: OrderBy
}
"""
order by max() on columns of table "simpipe.steps"
"""
input steps_max_order_by {
  "DateTime of when the step was created"
  created: OrderBy
  "DateTime of when the step was ended, NULL if not ended"
  ended: OrderBy
  "Docker image of the step"
  image: OrderBy
  "Name of the step, it is unique per run"
  name: OrderBy
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: OrderBy
  "UUID of the run, must exist in the runs table"
  runId: OrderBy
  "DateTime of when the step was started, NULL if not started"
  started: OrderBy
  "UUID of the step, random by default"
  stepId: OrderBy
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: OrderBy
}
"""
order by min() on columns of table "simpipe.steps"
"""
input steps_min_order_by {
  "DateTime of when the step was created"
  created: OrderBy
  "DateTime of when the step was ended, NULL if not ended"
  ended: OrderBy
  "Docker image of the step"
  image: OrderBy
  "Name of the step, it is unique per run"
  name: OrderBy
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: OrderBy
  "UUID of the run, must exist in the runs table"
  runId: OrderBy
  "DateTime of when the step was started, NULL if not started"
  started: OrderBy
  "UUID of the step, random by default"
  stepId: OrderBy
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: OrderBy
}
"""
order by stddev() on columns of table "simpipe.steps"
"""
input steps_stddev_order_by {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: OrderBy
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: OrderBy
}
"""
order by stddev_pop() on columns of table "simpipe.steps"
"""
input steps_stddev_pop_order_by {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: OrderBy
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: OrderBy
}
"""
order by stddev_samp() on columns of table "simpipe.steps"
"""
input steps_stddev_samp_order_by {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: OrderBy
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: OrderBy
}
"""
order by sum() on columns of table "simpipe.steps"
"""
input steps_sum_order_by {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: OrderBy
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: OrderBy
}
"""
order by var_pop() on columns of table "simpipe.steps"
"""
input steps_var_pop_order_by {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: OrderBy
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: OrderBy
}
"""
order by var_samp() on columns of table "simpipe.steps"
"""
input steps_var_samp_order_by {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: OrderBy
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: OrderBy
}
"""
order by variance() on columns of table "simpipe.steps"
"""
input steps_variance_order_by {
  "Step number in the pipeline, must be unique per run and a positive integer"
  pipelineStepNumber: OrderBy
  "Timeout of the step in seconds, must be between 1 and 86400"
  timeout: OrderBy
}
type subscription_root {
  "An array relationship"
  cpu("distinct select on columns" distinctOn: [cpuSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [cpuOrderBy!], "filter the rows returned" where: cpuBoolExp): [cpu!]!
  "An aggregate relationship"
  cpuAggregate("distinct select on columns" distinctOn: [cpuSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [cpuOrderBy!], "filter the rows returned" where: cpuBoolExp): cpuAggregate!
  """
  fetch data from the table in a streaming manner: "simpipe.cpu"
  """
  cpuStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [cpuStreamCursorInput]!, "filter the rows returned" where: cpuBoolExp): [cpu!]!
  "An array relationship"
  fsReadsMerged("distinct select on columns" distinctOn: [fsReadsMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsReadsMergedOrderBy!], "filter the rows returned" where: fsReadsMergedBoolExp): [fsReadsMerged!]!
  "An aggregate relationship"
  fsReadsMergedAggregate("distinct select on columns" distinctOn: [fsReadsMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsReadsMergedOrderBy!], "filter the rows returned" where: fsReadsMergedBoolExp): fsReadsMergedAggregate!
  """
  fetch data from the table in a streaming manner: "simpipe.fs_reads_merged"
  """
  fsReadsMergedStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [fsReadsMergedStreamCursorInput]!, "filter the rows returned" where: fsReadsMergedBoolExp): [fsReadsMerged!]!
  "An array relationship"
  fsWritesMerged("distinct select on columns" distinctOn: [fsWritesMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsWritesMergedOrderBy!], "filter the rows returned" where: fsWritesMergedBoolExp): [fsWritesMerged!]!
  "An aggregate relationship"
  fsWritesMergedAggregate("distinct select on columns" distinctOn: [fsWritesMergedSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [fsWritesMergedOrderBy!], "filter the rows returned" where: fsWritesMergedBoolExp): fsWritesMergedAggregate!
  """
  fetch data from the table in a streaming manner: "simpipe.fs_writes_merged"
  """
  fsWritesMergedStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [fsWritesMergedStreamCursorInput]!, "filter the rows returned" where: fsWritesMergedBoolExp): [fsWritesMerged!]!
  "An array relationship"
  memoryMaxUsageBytes("distinct select on columns" distinctOn: [memoryMaxUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryMaxUsageBytesOrderBy!], "filter the rows returned" where: memoryMaxUsageBytesBoolExp): [memoryMaxUsageBytes!]!
  "An aggregate relationship"
  memoryMaxUsageBytesAggregate("distinct select on columns" distinctOn: [memoryMaxUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryMaxUsageBytesOrderBy!], "filter the rows returned" where: memoryMaxUsageBytesBoolExp): memoryMaxUsageBytesAggregate!
  """
  fetch data from the table in a streaming manner: "simpipe.memory_max_usage_bytes"
  """
  memoryMaxUsageBytesStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [memoryMaxUsageBytesStreamCursorInput]!, "filter the rows returned" where: memoryMaxUsageBytesBoolExp): [memoryMaxUsageBytes!]!
  "An array relationship"
  memoryUsageBytes("distinct select on columns" distinctOn: [memoryUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryUsageBytesOrderBy!], "filter the rows returned" where: memoryUsageBytesBoolExp): [memoryUsageBytes!]!
  "An aggregate relationship"
  memoryUsageBytesAggregate("distinct select on columns" distinctOn: [memoryUsageBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [memoryUsageBytesOrderBy!], "filter the rows returned" where: memoryUsageBytesBoolExp): memoryUsageBytesAggregate!
  """
  fetch data from the table in a streaming manner: "simpipe.memory_usage_bytes"
  """
  memoryUsageBytesStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [memoryUsageBytesStreamCursorInput]!, "filter the rows returned" where: memoryUsageBytesBoolExp): [memoryUsageBytes!]!
  "An array relationship"
  networkReceivedBytes("distinct select on columns" distinctOn: [networkReceivedBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkReceivedBytesOrderBy!], "filter the rows returned" where: networkReceivedBytesBoolExp): [networkReceivedBytes!]!
  "An aggregate relationship"
  networkReceivedBytesAggregate("distinct select on columns" distinctOn: [networkReceivedBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkReceivedBytesOrderBy!], "filter the rows returned" where: networkReceivedBytesBoolExp): networkReceivedBytesAggregate!
  """
  fetch data from the table in a streaming manner: "simpipe.network_received_bytes"
  """
  networkReceivedBytesStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [networkReceivedBytesStreamCursorInput]!, "filter the rows returned" where: networkReceivedBytesBoolExp): [networkReceivedBytes!]!
  "An array relationship"
  networkTransmitBytes("distinct select on columns" distinctOn: [networkTransmitBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkTransmitBytesOrderBy!], "filter the rows returned" where: networkTransmitBytesBoolExp): [networkTransmitBytes!]!
  "An aggregate relationship"
  networkTransmitBytesAggregate("distinct select on columns" distinctOn: [networkTransmitBytesSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [networkTransmitBytesOrderBy!], "filter the rows returned" where: networkTransmitBytesBoolExp): networkTransmitBytesAggregate!
  """
  fetch data from the table in a streaming manner: "simpipe.network_transmit_bytes"
  """
  networkTransmitBytesStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [networkTransmitBytesStreamCursorInput]!, "filter the rows returned" where: networkTransmitBytesBoolExp): [networkTransmitBytes!]!
  """
  fetch data from the table: "simpipe.runs" using primary key columns
  """
  run("UUID of the run, random by default" runId: uuid!): runs
  "An array relationship"
  runs("distinct select on columns" distinctOn: [runsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [runsOrderBy!], "filter the rows returned" where: runsBoolExp): [runs!]!
  "An aggregate relationship"
  runsAggregate("distinct select on columns" distinctOn: [runsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [runsOrderBy!], "filter the rows returned" where: runsBoolExp): runsAggregate!
  """
  fetch data from the table in a streaming manner: "simpipe.runs"
  """
  runsStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [runsStreamCursorInput]!, "filter the rows returned" where: runsBoolExp): [runs!]!
  """
  fetch data from the table: "simpipe.envs"
  """
  simpipeEnvs("distinct select on columns" distinctOn: [SimpipeEnvsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeEnvsOrderBy!], "filter the rows returned" where: SimpipeEnvsBoolExp): [SimpipeEnvs!]!
  """
  fetch aggregated fields from the table: "simpipe.envs"
  """
  simpipeEnvsAggregate("distinct select on columns" distinctOn: [SimpipeEnvsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeEnvsOrderBy!], "filter the rows returned" where: SimpipeEnvsBoolExp): SimpipeEnvsAggregate!
  """
  fetch data from the table: "simpipe.envs" using primary key columns
  """
  simpipeEnvsByPk("UUID of the env, random by default" envId: uuid!): SimpipeEnvs
  """
  fetch data from the table in a streaming manner: "simpipe.envs"
  """
  simpipeEnvsStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [SimpipeEnvsStreamCursorInput]!, "filter the rows returned" where: SimpipeEnvsBoolExp): [SimpipeEnvs!]!
  """
  fetch data from the table: "simpipe.logs"
  """
  simpipeLogs("distinct select on columns" distinctOn: [SimpipeLogsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeLogsOrderBy!], "filter the rows returned" where: SimpipeLogsBoolExp): [SimpipeLogs!]!
  """
  fetch aggregated fields from the table: "simpipe.logs"
  """
  simpipeLogsAggregate("distinct select on columns" distinctOn: [SimpipeLogsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeLogsOrderBy!], "filter the rows returned" where: SimpipeLogsBoolExp): SimpipeLogsAggregate!
  """
  fetch data from the table: "simpipe.logs" using primary key columns
  """
  simpipeLogsByPk("UUID of the step, must exist in the steps table" stepId: uuid!): SimpipeLogs
  """
  fetch data from the table in a streaming manner: "simpipe.logs"
  """
  simpipeLogsStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [SimpipeLogsStreamCursorInput]!, "filter the rows returned" where: SimpipeLogsBoolExp): [SimpipeLogs!]!
  """
  fetch data from the table: "simpipe.run_status"
  """
  simpipeRunStatus("distinct select on columns" distinctOn: [SimpipeRunStatusSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeRunStatusOrderBy!], "filter the rows returned" where: SimpipeRunStatusBoolExp): [SimpipeRunStatus!]!
  """
  fetch aggregated fields from the table: "simpipe.run_status"
  """
  simpipeRunStatusAggregate("distinct select on columns" distinctOn: [SimpipeRunStatusSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeRunStatusOrderBy!], "filter the rows returned" where: SimpipeRunStatusBoolExp): SimpipeRunStatusAggregate!
  """
  fetch data from the table: "simpipe.run_status" using primary key columns
  """
  simpipeRunStatusByPk(value: String!): SimpipeRunStatus
  """
  fetch data from the table in a streaming manner: "simpipe.run_status"
  """
  simpipeRunStatusStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [SimpipeRunStatusStreamCursorInput]!, "filter the rows returned" where: SimpipeRunStatusBoolExp): [SimpipeRunStatus!]!
  """
  fetch data from the table: "simpipe.step_status"
  """
  simpipeStepStatus("distinct select on columns" distinctOn: [SimpipeStepStatusSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeStepStatusOrderBy!], "filter the rows returned" where: SimpipeStepStatusBoolExp): [SimpipeStepStatus!]!
  """
  fetch aggregated fields from the table: "simpipe.step_status"
  """
  simpipeStepStatusAggregate("distinct select on columns" distinctOn: [SimpipeStepStatusSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [SimpipeStepStatusOrderBy!], "filter the rows returned" where: SimpipeStepStatusBoolExp): SimpipeStepStatusAggregate!
  """
  fetch data from the table: "simpipe.step_status" using primary key columns
  """
  simpipeStepStatusByPk(value: String!): SimpipeStepStatus
  """
  fetch data from the table in a streaming manner: "simpipe.step_status"
  """
  simpipeStepStatusStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [SimpipeStepStatusStreamCursorInput]!, "filter the rows returned" where: SimpipeStepStatusBoolExp): [SimpipeStepStatus!]!
  """
  fetch data from the table: "simpipe.simulations" using primary key columns
  """
  simulation("UUID of the simulation, random by default" simulationId: uuid!): simulations
  """
  fetch data from the table: "simpipe.simulations"
  """
  simulations("distinct select on columns" distinctOn: [simulationsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [simulationsOrderBy!], "filter the rows returned" where: simulationsBoolExp): [simulations!]!
  """
  fetch aggregated fields from the table: "simpipe.simulations"
  """
  simulationsAggregate("distinct select on columns" distinctOn: [simulationsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [simulationsOrderBy!], "filter the rows returned" where: simulationsBoolExp): simulationsAggregate!
  """
  fetch data from the table in a streaming manner: "simpipe.simulations"
  """
  simulationsStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [simulationsStreamCursorInput]!, "filter the rows returned" where: simulationsBoolExp): [simulations!]!
  "An array relationship"
  steps("distinct select on columns" distinctOn: [stepsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [stepsOrderBy!], "filter the rows returned" where: stepsBoolExp): [steps!]!
  "An aggregate relationship"
  stepsAggregate("distinct select on columns" distinctOn: [stepsSelectColumn!], "limit the number of rows returned" limit: Int, "skip the first n rows. Use only with order_by" offset: Int, "sort the rows by one or more columns" orderBy: [stepsOrderBy!], "filter the rows returned" where: stepsBoolExp): stepsAggregate!
  """
  fetch data from the table: "simpipe.steps" using primary key columns
  """
  stepsByPk("UUID of the step, random by default" stepId: uuid!): steps
  """
  fetch data from the table in a streaming manner: "simpipe.steps"
  """
  stepsStream("maximum number of rows returned in a single batch" batchSize: Int!, "cursor to stream the results returned by the query" cursor: [stepsStreamCursorInput]!, "filter the rows returned" where: stepsBoolExp): [steps!]!
}
scalar timestamp
scalar timestamptz
scalar uuid
